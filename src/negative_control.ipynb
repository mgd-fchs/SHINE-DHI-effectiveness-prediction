{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict \"Responsiveness\" in the Control Group\n",
    "\n",
    "To ensure that the effect we find can be associated with the intervention and not other factors of reduced drinking, we assign participants from the control randomly to \"on\" and \"off\" weeks to check if we can make predictions about their reduced drinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import joblib \n",
    "from itertools import product, combinations, chain\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, mannwhitneyu\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing and model evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, recall_score,\n",
    "    precision_recall_curve, confusion_matrix, auc, precision_score,\n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Model interpretation\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define threshold for responsiveness\n",
    "\n",
    "Indicate change threshold that qualifies a participant as responsive vs non-responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE RESPONSIVENESS\n",
    "# avg reduction in drinking occasions between active and control weeks\n",
    "def_response_drink_occasions = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_study1 = pd.read_csv('../data/intervention_time/osf_study1.csv')\n",
    "data_study2 = pd.read_csv('../data/intervention_time/osf_study2.csv')\n",
    "\n",
    "# Study 1 baseline data (train/val input)\n",
    "b1_alcohol_self = pd.read_csv('../data/baseline/alcoholself_bucket280225.csv', index_col=0)\n",
    "b2_group_subjective = pd.read_csv('../data/baseline/subjective_grouperceptions_280225.csv', index_col=0)\n",
    "b3_group_sociometric = pd.read_csv('../data/baseline/data_social.csv')\n",
    "b4_brain = pd.read_csv('../data/baseline/brain_bucket_280225.csv', index_col=0)\n",
    "b5_demographic = pd.read_csv('../data/baseline/demographic_bucket280225.csv', index_col=0)\n",
    "b6_psychometric = pd.read_csv('../data/baseline/psychometrics_bucket280225.csv', index_col=0)\n",
    "\n",
    "# Study 2 subjective data (test input)\n",
    "b2_group_subjective_study2 = pd.read_csv('../data/baseline/subjective_grouperceptions_test.csv')\n",
    "\n",
    "# Sample characteristics\n",
    "baseline_demo_study2 = pd.read_csv('../data/baseline/demo_study2_full.csv')\n",
    "baseline_demo_study1 = pd.read_csv('../data/baseline/demo_study1_full.csv')\n",
    "baseline_auq_study1 = pd.read_csv('../data/baseline/study_1_all_drinking.csv')\n",
    "\n",
    "# Study 1 & 2 drinking/responsiveness data (output -> prediction target)\n",
    "if def_response_drink_occasions == -1:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -0.5:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-0_5.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -2:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-2.csv', index_col=0).reset_index()\n",
    "\n",
    "responsive_study2 = pd.read_csv('../data/intervention_time/responsiveness_study2.csv', index_col=0).reset_index()\n",
    "\n",
    "# All subjective group perceptions\n",
    "b2_group_subjective = pd.read_csv('../data/baseline/subjective_grouperceptions_all.csv', index_col=0)\n",
    "\n",
    "control_ppl = pd.read_csv('../data/added_analysis/weekly_drinking_summary_controlgroup.csv', index_col=0).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Characteristics\n",
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_study1_ids = pd.read_csv(\"../data/study_1_ids.csv\")\n",
    "active_study2_ids = pd.read_csv(\"../data/study_2_ids.csv\")\n",
    "\n",
    "control_summary_df = pd.read_csv(\"../data/intervention_time/intervention_time_controls_study1and2.csv\")\n",
    "summary_stats = control_summary_df.agg({\n",
    "    'adherence': ['mean', 'std', 'min', 'max'],\n",
    "    'drinking_occasions': ['mean', 'std', 'min', 'max'],\n",
    "    'avg_drinks_per_occasion': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasions: 1.46, 1.52, (0.0-6.25)\n",
    "Amount: 2.04, 1.86, (0.0-8.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1_control_subset = control_summary_df[control_summary_df['id'].isin(data_study1['id'])]\n",
    "study1_control_ids = study1_control_subset[\"id\"]\n",
    "\n",
    "summary_stats = study1_control_subset.agg({\n",
    "    'adherence': ['mean', 'std', 'min', 'max'],\n",
    "    'drinking_occasions': ['mean', 'std', 'min', 'max'],\n",
    "    'avg_drinks_per_occasion': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasions: 2.2, 1.59, (0.0-6.25)\n",
    "Amount: 3.6, 1.86, (0.0-8.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study2_control_subset = control_summary_df[control_summary_df['id'].isin(data_study2['id'])]\n",
    "study2_control_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1_control_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AUQ baselines\n",
    "\n",
    "baseline_subset = (\n",
    "    baseline_auq_study1\n",
    "    .loc[:, ['SID', 'AUQ_drink_amount', 'AUQ_drink_frequency']]\n",
    "    .rename(columns={'SID': 'id'})\n",
    ")\n",
    "\n",
    "baseline_subset = baseline_subset[baseline_subset['id'].isin(data_study1['id'])]\n",
    "\n",
    "active_df = baseline_subset[baseline_subset['id'].isin(active_study1_ids['id'])]\n",
    "control_df = baseline_subset[\n",
    "    (~baseline_subset['id'].isin(active_study1_ids['id'])) &\n",
    "    (baseline_subset['id'].isin(study1_control_ids))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats\n",
    "desc = {}\n",
    "for group_name, df in {'active': active_df, 'control': control_df}.items():\n",
    "    desc[group_name] = df[['AUQ_drink_amount', 'AUQ_drink_frequency']].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "\n",
    "desc_table = pd.concat(desc, axis=1)\n",
    "print(desc_table)\n",
    "\n",
    "# Statistical tests\n",
    "for var in ['AUQ_drink_amount', 'AUQ_drink_frequency']:\n",
    "    t, p = stats.ttest_ind(active_df[var], control_df[var], nan_policy='omit', equal_var=False)\n",
    "    print(f\"{var}: t = {t:.2f}, p = {p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_auq_study1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender = 1 is Male\n",
    "baseline_demo_study1 = (\n",
    "    baseline_demo_study1[['pID', 'age', 'gender_numeric', 'race_numeric']]\n",
    "    .rename(columns={'pID': 'id'}, errors='ignore')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_demo_study1 = baseline_demo_study1[baseline_demo_study1['id'].isin(data_study1['id'])]\n",
    "\n",
    "active_df = baseline_demo_study1[baseline_demo_study1['id'].isin(active_study1_ids['id'])]\n",
    "control_df = baseline_demo_study1[\n",
    "    (~baseline_demo_study1['id'].isin(active_study1_ids['id'])) &\n",
    "    (baseline_demo_study1['id'].isin(study1_control_ids))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_group(df, name):\n",
    "    print(f\"\\n===== {name} group =====\")\n",
    "    # Age\n",
    "    age_mean = df['age'].mean()\n",
    "    age_sd = df['age'].std()\n",
    "    age_min = df['age'].min()\n",
    "    age_max = df['age'].max()\n",
    "    print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "\n",
    "    # Gender distribution\n",
    "    gender_counts = df['gender_numeric'].value_counts(dropna=False).sort_index()\n",
    "    gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "    print(\"Gender distribution:\")\n",
    "    for g, c in gender_counts.items():\n",
    "        label = \"Missing\" if pd.isna(g) else f\"Gender {g}\"\n",
    "        print(f\"  {label}: {c} ({gender_percent[g]}%)\")\n",
    "\n",
    "    # Race distribution\n",
    "    race_counts = df['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "    race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "    print(\"Race distribution:\")\n",
    "    for r, c in race_counts.items():\n",
    "        label = \"Missing\" if pd.isna(r) else f\"Race {r}\"\n",
    "        print(f\"  {label}: {c} ({race_percent[r]}%)\")\n",
    "\n",
    "# Descriptives for each group\n",
    "describe_group(active_df, \"Active\")\n",
    "describe_group(control_df, \"Control\")\n",
    "\n",
    "# ---- Statistical comparisons ----\n",
    "print(\"\\n===== Statistical comparisons =====\")\n",
    "\n",
    "# Continuous variable: Age (Welch t-test)\n",
    "t_age, p_age = stats.ttest_ind(\n",
    "    active_df['age'], control_df['age'],\n",
    "    nan_policy='omit', equal_var=False\n",
    ")\n",
    "print(f\"Age: t = {t_age:.2f}, p = {p_age:.3f}\")\n",
    "\n",
    "# Categorical variable: Gender (Chi-square)\n",
    "gender_table = pd.crosstab(\n",
    "    baseline_demo_study1['gender_numeric'],\n",
    "    baseline_demo_study1['id'].isin(active_study1_ids['id'])\n",
    ")\n",
    "chi2_g, p_g, dof_g, _ = stats.chi2_contingency(gender_table)\n",
    "print(f\"Gender: χ²({dof_g}) = {chi2_g:.2f}, p = {p_g:.3f}\")\n",
    "\n",
    "# Categorical variable: Race (Chi-square)\n",
    "race_table = pd.crosstab(\n",
    "    baseline_demo_study1['race_numeric'],\n",
    "    baseline_demo_study1['id'].isin(active_study1_ids['id'])\n",
    ")\n",
    "chi2_r, p_r, dof_r, _ = stats.chi2_contingency(race_table)\n",
    "print(f\"Race: χ²({dof_r}) = {chi2_r:.2f}, p = {p_r:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study2_control_subset = control_summary_df[control_summary_df['id'].isin(data_study2['id'])]\n",
    "study2_control_ids = study2_control_subset[\"id\"]\n",
    "\n",
    "summary_stats = study2_control_subset.agg({\n",
    "    'adherence': ['mean', 'std', 'min', 'max'],\n",
    "    'drinking_occasions': ['mean', 'std', 'min', 'max'],\n",
    "    'avg_drinks_per_occasion': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasions: 1.07, 1.34, (0.0-5.25)\n",
    "Amount: 1.23, 1.25, (0.0-5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy coding weeks as active or control WITHIN THE CONTROL GROUP - calculate 'responsiveness' when not under treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define active/control weeks for each condition\n",
    "ACAC_active_weeks = [1, 3]\n",
    "ACAC_control_weeks = [2, 4]\n",
    "\n",
    "CACA_active_weeks = [2, 4]\n",
    "CACA_control_weeks = [1, 3]\n",
    "\n",
    "def compute_responsiveness(df, active_weeks, control_weeks):\n",
    "    active_avg = df[df['week'].isin(active_weeks)].groupby('id')['drinking_occasions'].mean()\n",
    "    control_avg = df[df['week'].isin(control_weeks)].groupby('id')['drinking_occasions'].mean()\n",
    "\n",
    "    comparison = pd.DataFrame({'active_avg': active_avg, 'control_avg': control_avg})\n",
    "    comparison['responsive'] = (comparison['active_avg'] - comparison['control_avg'] <= -1).astype(int)\n",
    "    return comparison.reset_index()\n",
    "\n",
    "# Apply for both conditions\n",
    "condition_ACAC = compute_responsiveness(control_ppl, ACAC_active_weeks, ACAC_control_weeks)\n",
    "condition_CACA = compute_responsiveness(control_ppl, CACA_active_weeks, CACA_control_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_ACAC\n",
    "condition_CACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Compute difference and descriptive stats\n",
    "for name, df in {'ACAC': condition_ACAC, 'CACA': condition_CACA}.items():\n",
    "    df['diff'] = df['control_avg'] - df['active_avg']\n",
    "    mean_diff = df['diff'].mean()\n",
    "    std_diff = df['diff'].std()\n",
    "    print(f\"{name} — Mean Δ(control−active): {mean_diff:.2f}, SD: {std_diff:.2f}\")\n",
    "\n",
    "# Combine both for plotting\n",
    "plot_df = (\n",
    "    pd.concat([\n",
    "        condition_ACAC.assign(condition='ACAC'),\n",
    "        condition_CACA.assign(condition='CACA')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Plot\n",
    "sns.set(style='whitegrid', palette='muted')\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=plot_df,\n",
    "    x='condition',\n",
    "    y='diff',\n",
    "    width=0.4,\n",
    "    showfliers=False,\n",
    "    boxprops=dict(alpha=0.6)\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=plot_df,\n",
    "    x='condition',\n",
    "    y='diff',\n",
    "    color='black',\n",
    "    alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "\n",
    "# Reference and threshold lines\n",
    "plt.axhline(0, color='black', linestyle='-', lw=1)\n",
    "for thresh in [-0.5, -1.0]:\n",
    "    plt.axhline(y=thresh, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.text(1.05, thresh + 0.05, f'{thresh}', color='grey', fontsize=9)\n",
    "\n",
    "plt.title('Difference in Drinking Occasions (Active − Control)', fontsize=14)\n",
    "plt.ylabel('Δ Control − Active (drinking occasions/week)')\n",
    "plt.xlabel('Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(condition_ACAC.responsive.sum())\n",
    "len(condition_ACAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(condition_CACA.responsive.sum())\n",
    "len(condition_CACA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_CACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_VARS = ['active_avg', 'control_avg']\n",
    "\n",
    "condition_ACAC.drop(columns=EXCLUDE_VARS, inplace=True)\n",
    "condition_CACA.drop(columns=EXCLUDE_VARS, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with `CONDITION_TO_TEST = condition_CACA` to test \"off-on-off-on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDITION_TO_TEST = condition_ACAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split based on the 'responsive' column\n",
    "responsive_study1, responsive_study2 = train_test_split(\n",
    "    CONDITION_TO_TEST,\n",
    "    test_size=0.3,\n",
    "    stratify=CONDITION_TO_TEST['responsive'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Baseline and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets -> Study 1\n",
    "b2_group_subjective_response = pd.merge(b2_group_subjective, responsive_study1, on='id', how='inner')\n",
    "\n",
    "print(f'Total IDs Study 1: {len(b2_group_subjective_response)}')\n",
    "print(f'Responsive IDs Study 1: {b2_group_subjective_response[b2_group_subjective_response[\"responsive\"] == 1][\"id\"].nunique()}')\n",
    "print('----------')\n",
    "\n",
    "# Testing dataset -> Study 2\n",
    "b2_group_subjective_test = pd.merge(b2_group_subjective, responsive_study2, on='id', how='inner')\n",
    "print(f'Total IDs Study 2: {len(b2_group_subjective_test)}')\n",
    "print(f'Responsive IDs Study 2: {b2_group_subjective_test[b2_group_subjective_test[\"responsive\"] == 1][\"id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'group_sub': b2_group_subjective_response\n",
    "}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Missing values in '{key}':\")\n",
    "    print(df.isna().sum())\n",
    "    print()  # for spacing between outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find highly correlated features within buckets\n",
    "Find redundancy in features if they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_group_subjective_response.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'responsive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highly_correlated_features(dataframes, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Identifies pairs of highly correlated features in each dataframe.\n",
    "    :param dataframes: dict of {name: dataframe}\n",
    "    :param threshold: correlation threshold to consider as \"high\"\n",
    "    :return: dict of {name: list of correlated feature pairs}\n",
    "    \"\"\"\n",
    "    correlated_features = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Exclude COMMON_VARS from the correlation computation\n",
    "        columns_to_correlate = [col for col in df.columns if col != TARGET_VAR and col !='id']\n",
    "        \n",
    "        # Compute correlation matrix only for selected columns\n",
    "        corr_matrix = df[columns_to_correlate].corr().abs()\n",
    "        \n",
    "        # Select the upper triangle of the correlation matrix\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find pairs of features with correlation above the threshold\n",
    "        correlated_pairs = [\n",
    "            (col, idx, upper_triangle.loc[idx, col])\n",
    "            for col in upper_triangle.columns\n",
    "            for idx in upper_triangle.index\n",
    "            if upper_triangle.loc[idx, col] > threshold\n",
    "        ]\n",
    "        \n",
    "        # Store results for the current dataframe\n",
    "        correlated_features[name] = correlated_pairs\n",
    "\n",
    "    return correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8)\n",
    "\n",
    "# Display results\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features per category\n",
    "{key: df.shape[1] for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests: Features\n",
    "### Mann-Whitney U Tests\n",
    "\n",
    "Hypothesis test for non-normally distributed data to check which of the remaining features show the most (significant) difference between the two groups (responsive vs non-responsive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mann_whitney_u(df, target_var, exclude_vars):\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_vars and col != target_var:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                group1 = df[df[target_var] == 0][col]\n",
    "                group2 = df[df[target_var] == 1][col]\n",
    "                stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                results[col] = {'U_statistic': stat, 'p_value': p_value}\n",
    "            except Exception as e:\n",
    "                results[col] = {'error': str(e)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_results = {}\n",
    "for name, df in dataframes.items():\n",
    "    if name != 'demo' and TARGET_VAR in df.columns:\n",
    "        mwu_results[name] = perform_mann_whitney_u(df, TARGET_VAR, 'id')\n",
    "\n",
    "# Output summary\n",
    "for name, results in mwu_results.items():\n",
    "    print(f\"\\n{name} DataFrame Mann-Whitney U Test Results (p-value < 0.05):\")\n",
    "    \n",
    "    # Ensure only variables with p-values < 0.05 are retained\n",
    "    significant_results = {}\n",
    "    for var, stats in results.items():\n",
    "        if isinstance(stats, dict) and 'p_value' in stats and stats['p_value'] < 0.05:\n",
    "            significant_results[var] = stats  \n",
    "    \n",
    "    if significant_results:\n",
    "        df_significant = pd.DataFrame(significant_results).T  \n",
    "        print(df_significant)\n",
    "    else:\n",
    "        print(\"No significant results (p-value < 0.05) found.\")\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features_and_targets(df, test_set=0, resampling=None):\n",
    "    if TARGET_VAR not in df.columns:\n",
    "        raise ValueError(f\"Target variable '{TARGET_VAR}' not found in dataframe.\")\n",
    "\n",
    "    # Extract target variable and drop excluded columns\n",
    "    targets = df[TARGET_VAR]\n",
    "    features = df[[col for col in df.columns if col != TARGET_VAR and col != 'id']]\n",
    "    features = features.drop(columns=[TARGET_VAR], errors='ignore')\n",
    "\n",
    "    # Split into training and test sets (STRATIFIED)\n",
    "    if test_set:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            features, targets, test_size=test_set, stratify=targets\n",
    "        )\n",
    "    else: \n",
    "        X_train = features\n",
    "        Y_train = targets\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "    # Median imputation for 'income_numeric' if it contains NA values\n",
    "    if 'income_numeric' in X_train.columns:\n",
    "        if X_train['income_numeric'].isna().any():\n",
    "            X_train['income_numeric'].fillna(X_train['income_numeric'].median(), inplace=True)\n",
    "        if isinstance(X_test, pd.DataFrame) and 'income_numeric' in X_test.columns and X_test['income_numeric'].isna().any():\n",
    "            X_test['income_numeric'].fillna(X_test['income_numeric'].median(), inplace=True)\n",
    "\n",
    "    if 'IAS_mean' in X_train.columns:\n",
    "        if X_train['IAS_mean'].isna().any():\n",
    "            X_train['IAS_mean'].fillna(X_train['IAS_mean'].median(), inplace=True)\n",
    "        if isinstance(X_test, pd.DataFrame) and 'IAS_mean' in X_test.columns and X_test['IAS_mean'].isna().any():\n",
    "            X_test['IAS_mean'].fillna(X_test['IAS_mean'].median(), inplace=True)\n",
    "\n",
    "    # TODO: Handle all other missingness here\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_kfold_grid_search(\n",
    "    X, Y, param_grid, k=5, CV_reps=1, eval_metric=['auc'], model_choice_metric='auc', \n",
    "    res_dir=\".\", model_type='rf', combo='alcohol'\n",
    "):\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "\n",
    "    # Initialize variables to store the best model and scores\n",
    "    best_model = None\n",
    "    best_scores = None\n",
    "    best_params = None\n",
    "    best_model_choice_value = -np.inf  # Track the best model based on the chosen metric\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    for params in param_combinations:\n",
    "        current_params = dict(zip(param_names, params))\n",
    "\n",
    "        # Store all fold results\n",
    "        all_folds_metrics = {metric: [] for metric in eval_metric}\n",
    "\n",
    "        for train_index, test_index in kf.split(X, Y):  # k-fold cv split\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "            rep_metrics = {metric: [] for metric in eval_metric}  # Reset for each fold\n",
    "\n",
    "            for _ in range(CV_reps):  # Repeat that split j times\n",
    "\n",
    "                # Initialize the model with the current parameters\n",
    "                if model_type == 'rf':\n",
    "                    model = RandomForestClassifier(\n",
    "                        n_estimators=current_params.get(\"n_estimators\", 100),\n",
    "                        max_depth=current_params.get(\"max_depth\"),\n",
    "                        min_samples_split=current_params.get(\"min_samples_split\", 2),\n",
    "                        min_samples_leaf=current_params.get(\"min_samples_leaf\", 1),\n",
    "                        class_weight=\"balanced\"\n",
    "                    )\n",
    "                elif model_type == 'xgb':\n",
    "                    model = XGBClassifier(\n",
    "                        n_estimators=current_params.get(\"n_estimators\", 100),\n",
    "                        max_depth=current_params.get(\"max_depth\", 6),\n",
    "                        learning_rate=current_params.get(\"learning_rate\", 0.1),\n",
    "                        min_child_weight=current_params.get(\"min_child_weight\", 1),\n",
    "                        gamma=current_params.get(\"gamma\", 0),\n",
    "                        subsample=current_params.get(\"subsample\", 1),\n",
    "                        colsample_bytree=current_params.get(\"colsample_bytree\", 1),\n",
    "                        scale_pos_weight=current_params.get(\"scale_pos_weight\", 1),\n",
    "                        use_label_encoder=False,\n",
    "                        eval_metric=\"logloss\"\n",
    "                    )\n",
    "\n",
    "                model.fit(X_train, Y_train)\n",
    "                Y_pred = model.predict(X_test)\n",
    "                Y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "                if 'auc' in eval_metric and Y_prob is not None:\n",
    "                    rep_metrics['auc'].append(roc_auc_score(Y_test, Y_prob))\n",
    "                if 'f1' in eval_metric:\n",
    "                    rep_metrics['f1'].append(f1_score(Y_test, Y_pred))\n",
    "                if 'accuracy' in eval_metric:\n",
    "                    rep_metrics['accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "                if 'specificity' in eval_metric or 'sensitivity' in eval_metric:\n",
    "                    tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "                    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "                    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "                    if 'specificity' in eval_metric:\n",
    "                        rep_metrics['specificity'].append(specificity)\n",
    "                    if 'sensitivity' in eval_metric:\n",
    "                        rep_metrics['sensitivity'].append(sensitivity)\n",
    "                if 'mcc' in eval_metric:\n",
    "                    rep_metrics['mcc'].append(matthews_corrcoef(Y_test, Y_pred))\n",
    "                if 'balancedAcc' in eval_metric:\n",
    "                    rep_metrics['balancedAcc'].append(balanced_accuracy_score(Y_test, Y_pred))\n",
    "                if 'pr_auc' in eval_metric and Y_prob is not None:\n",
    "                    precision, recall, _ = precision_recall_curve(Y_test, Y_prob)\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    rep_metrics['pr_auc'].append(pr_auc)\n",
    "\n",
    "            # Compute median scores per fold and store results\n",
    "            fold_median_metrics = {metric: np.mean(values) for metric, values in rep_metrics.items()}\n",
    "            for metric in eval_metric:\n",
    "                all_folds_metrics[metric].append(fold_median_metrics[metric])\n",
    "\n",
    "        # Compute final median scores over all folds\n",
    "        median_rep_metrics = {metric: np.mean(values) for metric, values in all_folds_metrics.items()}\n",
    "\n",
    "        # Select best model based on median of model_choice_metric\n",
    "        if median_rep_metrics[model_choice_metric] > best_model_choice_value:\n",
    "            best_model_choice_value = median_rep_metrics[model_choice_metric]\n",
    "            best_model = model\n",
    "            best_params = current_params\n",
    "            best_scores = median_rep_metrics  # Store median scores for all metrics\n",
    "\n",
    "    return best_model, best_scores, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_csv(results_dict, results_dir, filename):\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Define the output file path\n",
    "    file_path = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Extract all metric names\n",
    "    all_metrics = set()\n",
    "    for metrics in results_dict.values():\n",
    "        all_metrics.update(metrics.keys())\n",
    "    \n",
    "    # Sort metrics for consistency\n",
    "    all_metrics = sorted(all_metrics)\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(file_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header\n",
    "        header = [\"run\", \"group\"] + all_metrics\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write data\n",
    "        for group, metrics in results_dict.items():\n",
    "            num_runs = len(next(iter(metrics.values())))  # Get number of runs from first metric\n",
    "            for run_idx in range(num_runs):\n",
    "                row = [run_idx, str(group)]  # Start with run index and group name\n",
    "                for metric in all_metrics:\n",
    "                    value = metrics.get(metric, [np.nan] * num_runs)[run_idx]  # Handle missing values\n",
    "                    row.append(value)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores):\n",
    "    Y_test_flat = Y_test.ravel()\n",
    "    \n",
    "    test_scores['auc'].append(roc_auc_score(Y_test_flat, proba_predictions))\n",
    "    test_scores['f1'].append(f1_score(Y_test_flat, test_predictions))\n",
    "    test_scores['accuracy'].append(accuracy_score(Y_test_flat, test_predictions))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test_flat, test_predictions).ravel()\n",
    "    test_scores['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else np.nan)\n",
    "    test_scores['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else np.nan)\n",
    "    test_scores['PPV'].append(tp / (tp + fp) if (tp + fp) > 0 else np.nan)\n",
    "    test_scores['NPV'].append(tn / (tn + fn) if (tn + fn) > 0 else np.nan)\n",
    "    test_scores['MCC'].append(matthews_corrcoef(Y_test_flat, test_predictions))\n",
    "    test_scores['balancedAcc'].append(balanced_accuracy_score(Y_test_flat, test_predictions))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(Y_test_flat, proba_predictions)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    test_scores['pr_auc'].append(pr_auc)\n",
    "\n",
    "    test_scores['tn'].append(tn)\n",
    "    test_scores['fp'].append(fp)\n",
    "    test_scores['fn'].append(fn)\n",
    "    test_scores['tp'].append(tp)\n",
    "\n",
    "    return test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_score_dict(score_dict, res_dir=None, filename=None):\n",
    "    rows = []\n",
    "    for combination, metrics in score_dict.items():\n",
    "        row = {\"Combination\": combination}\n",
    "        for metric, values in metrics.items():\n",
    "            row[f\"{metric}_mean\"] = values[\"mean\"]\n",
    "            row[f\"{metric}_CI_lower\"] = values[\"95%_CI\"][0]\n",
    "            row[f\"{metric}_CI_upper\"] = values[\"95%_CI\"][1]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_comb = pd.DataFrame(df[\"Combination\"].tolist(), columns=[f\"Factor_{i+1}\" for i in range(df[\"Combination\"].map(len).max())])\n",
    "    df = pd.concat([df_comb, df.drop(columns=\"Combination\")], axis=1)\n",
    "\n",
    "    if res_dir and filename:\n",
    "        df.to_csv(f\"{res_dir}/{filename}\", index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo):\n",
    "    # Combine SHAP values and test data\n",
    "    final_shap_values = np.vstack(all_shap_values)\n",
    "    final_test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "    # Compute relative importance\n",
    "    mean_abs_shap = np.abs(final_shap_values).mean(axis=0)\n",
    "    rel_importance = 100 * mean_abs_shap / mean_abs_shap.sum()\n",
    "\n",
    "    # Plot SHAP summary without showing\n",
    "    plt.figure()\n",
    "    shap.summary_plot(final_shap_values, final_test_data, show=False, cmap='winter')\n",
    "\n",
    "    # Get current axis and y-tick labels\n",
    "    ax = plt.gca()\n",
    "    feature_names = [tick.get_text() for tick in ax.get_yticklabels()]\n",
    "\n",
    "    # Use Index.get_loc instead of list\n",
    "    col_index = final_test_data.columns\n",
    "    feature_order = [col_index.get_loc(name) for name in feature_names]\n",
    "\n",
    "    # Add percentage values\n",
    "    percent_labels = [f\"{name} ({rel_importance[i]:.1f}%)\" for name, i in zip(feature_names, feature_order)]\n",
    "    ax.set_yticklabels(percent_labels, fontsize=10)\n",
    "\n",
    "    # Save updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{res_dir}/{combo}_shap_summary_plot_with_percentages.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Return top 2 most important features (by mean absolute SHAP)\n",
    "    top2_indices = np.argsort(mean_abs_shap)[-2:][::-1]\n",
    "    top2_features = final_test_data.columns[top2_indices].tolist()\n",
    "    return top2_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def plot_pdp_across_runs(best_model, res_dir, all_test_data, feature_names=None, interaction_pair=None, colors=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots PDPs with mean and std across multiple test sets for each feature.\n",
    "    Optionally adds an interaction plot.\n",
    "\n",
    "    Parameters:\n",
    "        best_model: trained model\n",
    "        all_test_data: list of pd.DataFrames used for PDP evaluation\n",
    "        feature_names: list of features to plot (default: all features in data)\n",
    "        interaction_pair: tuple of two features to plot interaction PDP\n",
    "        colors: optional color list\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = [\"#22223B\", \"#4A4E69\", \"#9A8C98\", \"#C9ADA7\", \"#F2E9E4\"]\n",
    "\n",
    "    final_test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = final_test_data.columns.tolist()\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    num_plots = num_features + (1 if interaction_pair else 0)\n",
    "    num_cols = 3\n",
    "    num_rows = -(-num_plots // num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature_name in enumerate(feature_names):\n",
    "        pdp_values = []\n",
    "        feature_values_list = []\n",
    "\n",
    "        for dat in all_test_data:\n",
    "            ax_dummy = plt.figure().add_subplot()\n",
    "            ax_dummy.set_visible(False)\n",
    "            pdp_display = PartialDependenceDisplay.from_estimator(best_model, dat, [feature_name], ax=ax_dummy)\n",
    "            plt.close(ax_dummy.figure)\n",
    "\n",
    "            pdp_x = pdp_display.lines_[0][0].get_xdata()\n",
    "            pdp_y = pdp_display.lines_[0][0].get_ydata()\n",
    "            pdp_values.append(pdp_y)\n",
    "            feature_values_list.append(pdp_x)\n",
    "\n",
    "        common_feature_values = np.linspace(min(map(min, feature_values_list)),\n",
    "                                            max(map(max, feature_values_list)), num=100)\n",
    "\n",
    "        interpolated_pdp_values = []\n",
    "        for i in range(len(pdp_values)):\n",
    "            f_interp = interp1d(feature_values_list[i], pdp_values[i], kind=\"linear\", fill_value=\"extrapolate\")\n",
    "            interpolated_pdp_values.append(f_interp(common_feature_values))\n",
    "\n",
    "        pdp_values = np.array(interpolated_pdp_values)\n",
    "        pdp_mean = np.mean(pdp_values, axis=0)\n",
    "        pdp_std = np.std(pdp_values, axis=0)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        ax.plot(common_feature_values, pdp_mean, label=\"Mean PDP\", color=colors[0], lw=2)\n",
    "        ax.fill_between(common_feature_values, pdp_mean - pdp_std, pdp_mean + pdp_std,\n",
    "                        color=colors[2], alpha=0.5, label=\"Std Dev\")\n",
    "        ax.set_ylabel(\"Predicted Value\")\n",
    "        ax.set_title(f\"PDP for {feature_name}\")\n",
    "        ax.legend()\n",
    "\n",
    "    if interaction_pair:\n",
    "        ax = axes[num_features]\n",
    "        PartialDependenceDisplay.from_estimator(best_model, final_test_data,\n",
    "                                                [interaction_pair], ax=ax)\n",
    "        ax.set_title(f\"Interaction: {interaction_pair[0]} & {interaction_pair[1]}\")\n",
    "\n",
    "    for i in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Build filename\n",
    "    interaction_suffix = f\"_{interaction_pair[0]}_{interaction_pair[1]}\" if interaction_pair else \"\"\n",
    "    if not title:\n",
    "        filename = f\"{res_dir}/pdp_plots{interaction_suffix}.png\"\n",
    "    else:\n",
    "        filename = f\"{res_dir}/pdp_plots{title}.png\"\n",
    "    # Save figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "def run_rf_train_test(dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5, model_choice_metric='f1', \n",
    "                      res_dir=f\"./results/\", model_type='xgb', test_set=0.3, resampling=None, permutation=False):\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_resampling{resampling}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    keys = list(dataframes.keys())\n",
    "\n",
    "    # combine data categories\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {} \n",
    "    best_shap_vals = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "        \n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "    \n",
    "        all_shap_values = []\n",
    "        all_test_data = []\n",
    "        best_overall_score = -np.inf \n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "        best_shap_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps): # i repetitions of train/test\n",
    "\n",
    "            # Prepare train/test split for this i (random & stratified)\n",
    "            X_data, Y_data, X_test, Y_test = prepare_features_and_targets(merged_df.copy(), test_set=test_set, resampling=resampling)\n",
    "\n",
    "            # Shuffle labels for permutation tests\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            else:\n",
    "                best_model, best_scores, best_params = random_forest_kfold_grid_search(X_data, Y_data, \n",
    "                                                                                    param_grid, k=k, \n",
    "                                                                                    CV_reps=CV_reps, \n",
    "                                                                                    eval_metric=eval_metrics,\n",
    "                                                                                    model_choice_metric=model_choice_metric,\n",
    "                                                                                    res_dir=res_dir,\n",
    "                                                                                    model_type=model_type,\n",
    "                                                                                    combo=combo)\n",
    "            # Collect metrics\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain the best model on the full training dataset and evaluate on the test set\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test) \n",
    "            shap_values = shap_values[:, :, 1]\n",
    "\n",
    "            # Append SHAP values and test data for later aggregation\n",
    "            all_shap_values.append(shap_values)\n",
    "            all_test_data.append(pd.DataFrame(X_test))\n",
    "\n",
    "            if best_scores[model_choice_metric] > best_overall_score:\n",
    "                best_overall_score = best_scores[model_choice_metric]\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "                best_shap_for_combo = shap_values  # Store SHAP values if needed\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores[model_choice_metric], deepcopy(best_model)))\n",
    "\n",
    "            # Calculate and append metrics for the test set\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep track of the best model based on the model_choice_metric\n",
    "        if combo not in best_models or best_scores[model_choice_metric] > combo_validation_scores[combo][model_choice_metric]['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "\n",
    "            best_shap_vals[combo] = best_shap_for_combo\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            # Save top 10 models for group_sub combo\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "\n",
    "                # Sort and save top 10 by score\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:10]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top10_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "\n",
    "                # Store back in locals so it's not overwritten each time\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        top2_features = plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo)\n",
    "\n",
    "        plot_pdp_across_runs(\n",
    "            best_model=best_model_for_combo,\n",
    "            res_dir=res_dir,\n",
    "            all_test_data=all_test_data,\n",
    "            interaction_pair=tuple(top2_features)\n",
    "        )\n",
    "\n",
    "        # Calculate mean and 95% CI for validation scores\n",
    "        z = norm.ppf(0.975)  # 95% confidence level\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        # Calculate mean and 95% CI for test scores\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        # For validation scores\n",
    "        df_val = flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        # For test scores\n",
    "        df_test = flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [2, 3, 5]\n",
    "}\n",
    "\n",
    "eval_metrics = ['auc', 'f1', 'accuracy', 'specificity', 'sensitivity', 'PPV', 'NPV', 'MCC', 'balancedAcc', 'pr_auc', 'tn', 'fn', 'tp', 'fp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rf_train_test(\n",
    "    dataframes=dataframes,\n",
    "    param_grid=param_grid,\n",
    "    eval_metrics=eval_metrics,\n",
    "    outer_reps=2, # reduce for faster run --> this affects the results\n",
    "    k=3,\n",
    "    CV_reps=5,\n",
    "    model_choice_metric='auc',\n",
    "    res_dir=\"../results/negative_control\",\n",
    "    model_type='rf',\n",
    "    test_set=0.3,\n",
    "    resampling=None,\n",
    "    permutation=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
