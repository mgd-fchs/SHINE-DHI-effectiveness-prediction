{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Intervention Responsiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook holds the code for the main analysis of the paper \"Peer Perceptions Emerge as Key Predictors in Multimodal Models of Digital Alcohol Intervention Effectiveness\" submitted to npj Digital Medicine.\n",
    "\n",
    "**Data modalities for the main analysis are:**\n",
    "- `b1_alcohol_self`: Self-reported individual alcohol use and related behaviors.\n",
    "- `b2_group_subjective`: Participant perceptions of their social group’s drinking norms, attitudes, and approval. How much a participant perceives their peers to drink.\n",
    "- `b3_group_sociometric`: Social network–derived measures of group structure and connections.\n",
    "- `b4_brain`: Preprocessed MRI-derived measures of brain activity and connectivity in alcohol-related tasks.\n",
    "- `b5_demographic`: Basic participant characteristics such as age, gender, and income.\n",
    "- `b6_psychometric`: Standardized questionnaire-based measures of psychological traits and states.\n",
    "\n",
    "**Additional analysis:**\n",
    "- `b7_objective_group_drinking`: Aggregated group-level drinking data (how much a participant's peers actually drink).\n",
    "(Analyses with this dataframe are commented - they are left in the code so as to make the processing of this data transparent. However, this analysis was added after the main analysis to compare the predictive utility of objective group drinking compared to group perceptions. This is reported in the supplements of the above paper.)\n",
    "\n",
    "**External test set:**\n",
    "- `b2_group_subjective_study2`: Participant perceptions of their social group’s drinking norms, attitudes, and approval in an independent sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from itertools import combinations, chain\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Visualization\n",
    "import shap\n",
    "\n",
    "# Serialization\n",
    "import joblib\n",
    "\n",
    "# Custom\n",
    "from pre_processing import *\n",
    "from training import *\n",
    "from plotting import *\n",
    "from testing import *\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed to replicate the exact results from the paper. Set to `None` to run without random seed (note that this can lead to small deviations in outcome metrics due to randomness in the CV and test process).\n",
    "\n",
    "Please note that the social network data (`b3_group_sociometric`) cannot be publicly provided due to privacy concerns. Running the script without this feature domain may cause some deviations in results from those reported in the main manuscript. Use and processing of this dataframe were left in the notebook but commented so that the processing of this data is still transparent. However, results are qualitatively reproducible and the code shows our procedure in processing and analysing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 321\n",
    "# SEED = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED:\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define threshold for responsiveness\n",
    "\n",
    "Indicate change threshold that qualifies a participant as responsive vs non-responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE RESPONSIVENESS\n",
    "# avg reduction in drinking occasions between active and control weeks\n",
    "def_response_drink_occasions = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../results\"\n",
    "\n",
    "data_study1 = pd.read_csv('../data/intervention_time/osf_study1.csv')\n",
    "data_study2 = pd.read_csv('../data/intervention_time/osf_study2.csv')\n",
    "\n",
    "# Study 1 baseline data (train/val input)\n",
    "b1_alcohol_self = pd.read_csv('../data/baseline/alcoholself_bucket280225.csv', index_col=0)\n",
    "b2_group_subjective = pd.read_csv('../data/baseline/subjective_grouperceptions_280225.csv', index_col=0)\n",
    "# b3_group_sociometric = pd.read_csv('../data/baseline/data_social.csv')\n",
    "b4_brain = pd.read_csv('../data/baseline/brain_bucket_280225.csv', index_col=0)\n",
    "b5_demographic = pd.read_csv('../data/baseline/demographic_bucket280225.csv', index_col=0)\n",
    "b6_psychometric = pd.read_csv('../data/baseline/psychometrics_bucket280225.csv', index_col=0)\n",
    "\n",
    "# # Added analysis - To evaluate performance of objective drinking metrics\n",
    "# b7_objective_group_drinking = pd.read_csv('../data/added_analysis/social_group_drinking.csv', index_col=0)\n",
    "\n",
    "# Study 2 peer perception data (test input)\n",
    "b2_group_subjective_study2 = pd.read_csv('../data/baseline/subjective_grouperceptions_test.csv')\n",
    "baseline_demo_study2 = pd.read_csv('../data/baseline/demo_study2_full.csv')\n",
    "\n",
    "# Study 1 & 2 drinking/responsiveness data (output -> prediction target)\n",
    "if def_response_drink_occasions == -1:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -0.5:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-0_5.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -2:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-2.csv', index_col=0).reset_index()\n",
    "\n",
    "responsive_study2 = pd.read_csv('../data/intervention_time/responsiveness_study2.csv', index_col=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_study1_control = data_study1[data_study1.condition == 'control']\n",
    "data_study2_control = data_study2[data_study2.condition == 'control']\n",
    "\n",
    "len(data_study1_control)\n",
    "len(data_study2_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates within each DataFrame\n",
    "duplicates_study1 = responsive_study1['id'].duplicated().any()\n",
    "duplicates_study2 = responsive_study2['id'].duplicated().any()\n",
    "\n",
    "print(f\"Study 1 has duplicates: {duplicates_study1}\")\n",
    "print(f\"Study 2 has duplicates: {duplicates_study2}\")\n",
    "\n",
    "# Check for overlapping IDs between the two studies\n",
    "ids_study1 = set(responsive_study1['id'])\n",
    "ids_study2 = set(responsive_study2['id'])\n",
    "overlap = ids_study1.intersection(ids_study2)\n",
    "\n",
    "print(f\"Number of overlapping IDs: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(f\"Overlapping IDs: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_VARS = [\n",
    "    'group', 'condition', 'active',\n",
    "    'control', 'difference_drinks_occasions']\n",
    "\n",
    "responsive_study1.drop(columns=EXCLUDE_VARS, inplace=True)\n",
    "responsive_study2.drop(columns=EXCLUDE_VARS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsive_study2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Baseline and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets -> Study 1\n",
    "b1_alcohol_self_response = pd.merge(b1_alcohol_self, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response = pd.merge(b2_group_subjective, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response_old = pd.merge(responsive_study1, responsive_study1, on='id', how='inner')\n",
    "# b3_group_sociometric_response = pd.merge(b3_group_sociometric, responsive_study1, on='id', how='inner')\n",
    "b4_brain_response = pd.merge(b4_brain, responsive_study1, on='id', how='inner')\n",
    "b5_demographic_response = pd.merge(b5_demographic, responsive_study1, on='id', how='inner')\n",
    "b6_psychometric_response = pd.merge(b6_psychometric, responsive_study1, on='id', how='inner')\n",
    "\n",
    "# b7_objective_group_drinking_response = pd.merge(b7_objective_group_drinking, responsive_study1, on='id', how='inner')\n",
    "\n",
    "print(f'Total IDs Study 1: {len(b1_alcohol_self_response)}')\n",
    "print(f'Responsive IDs Study 1: {b1_alcohol_self_response[b1_alcohol_self_response[\"responsive\"] == 1][\"id\"].nunique()}')\n",
    "print('----------')\n",
    "# Testing dataset -> Study 2\n",
    "b2_group_subjective_test = pd.merge(b2_group_subjective_study2, responsive_study2, on='id', how='inner')\n",
    "print(f'Total IDs Study 2: {len(b2_group_subjective_test)}')\n",
    "print(f'Responsive IDs Study 2: {b2_group_subjective_test[b2_group_subjective_test[\"responsive\"] == 1][\"id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    # 'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Missing values in '{key}':\")\n",
    "    print(df.isna().sum())\n",
    "    print()  # for spacing between outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find highly correlated features within buckets\n",
    "Find redundancy in features if they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    # 'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'responsive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8, target_var=TARGET_VAR)\n",
    "\n",
    "# Display results\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice is made manually \n",
    "\n",
    "dataframes['brain'].drop(columns=['reward', 'ROI_alc_react_v_rest_neurosynth_cogcontrol', 'ROI_alc_react_v_rest_neurosynth_craving', \\\n",
    "                                  'ROI_alc_react_v_rest_neurosynth_emoreg'], inplace=True)\n",
    "\n",
    "# dataframes['group_socio'].drop(columns=['leaders_deg_in', 'goToBad_deg_in'], inplace=True)\n",
    "\n",
    "dataframes['psych'].drop(columns=['ACS_focus', 'DERS_strategies', 'BIS_attention_total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all within-category correlations are gone\n",
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8)\n",
    "\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of remaining features per category\n",
    "{key: df.shape[1] for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Count initial number of unique IDs in each DataFrame\n",
    "print(\"Initial ID counts per dataframe:\")\n",
    "initial_counts = {name: df['id'].nunique() for name, df in dataframes.items()}\n",
    "for name, count in initial_counts.items():\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# 2) Identify and remove IDs with >10 missing values across all dataframes\n",
    "# Count missing values per ID across all dataframes\n",
    "missing_counts = Counter()\n",
    "for df in dataframes.values():\n",
    "    id_missing = df.set_index('id').isnull().sum(axis=1)\n",
    "    for idx, val in id_missing.items():\n",
    "        if val > 0:\n",
    "            missing_counts[idx] += val\n",
    "\n",
    "# Get IDs with >10 missing values in total\n",
    "bad_ids = {id_ for id_, miss_count in missing_counts.items() if miss_count > 10}\n",
    "\n",
    "# Drop those IDs from all dataframes\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][~dataframes[name]['id'].isin(bad_ids)]\n",
    "\n",
    "print(\"\\nID counts after removing IDs with >10 total missing features:\")\n",
    "post_clean_counts = {name: df['id'].nunique() for name, df in dataframes.items()}\n",
    "for name, count in post_clean_counts.items():\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# 3) Intersect IDs: keep only IDs present in all dataframes\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n",
    "\n",
    "# 4) Final N\n",
    "final_N = len(common_ids)\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n",
    "print(f\"\\nFinal number of participants present in all dataframes: N = {final_N}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Study 2\n",
    "b2_group_subjective_test\n",
    "\n",
    "# 1) Count initial number of unique IDs in each DataFrame\n",
    "print(\"Initial ID counts study 2:\")\n",
    "initial_counts = b2_group_subjective_test['id'].nunique()\n",
    "print(f\"  group_subjective_test: {initial_counts}\")\n",
    "\n",
    "# Remove rows with more than 1 missing feature\n",
    "b2_group_subjective_test = b2_group_subjective_test[b2_group_subjective_test.isnull().sum(axis=1) <= 1]\n",
    "\n",
    "# Check how many IDs remain\n",
    "remaining_ids = b2_group_subjective_test['id'].nunique()\n",
    "print(f\"  group_subjective_test (after removing >1 missing): {remaining_ids}\")\n",
    "print(f\"Responsive: {b2_group_subjective_test['responsive'].sum()} out of {b2_group_subjective_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique 'id's in each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    unique_count = df['id'].nunique()\n",
    "    print(f\"{name}: {unique_count} unique IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get common IDs\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "\n",
    "# Step 2: Filter each DataFrame to keep only rows with common IDs\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get common IDs\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "\n",
    "# Step 2: Filter demographics\n",
    "demo_filtered = dataframes['demo'][dataframes['demo']['id'].isin(common_ids)]\n",
    "\n",
    "# Step 3: Compute summaries\n",
    "N = demo_filtered.shape[0]\n",
    "age_mean = round(demo_filtered['age'].mean(), 2)\n",
    "age_sd = round(demo_filtered['age'].std(), 2)\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = demo_filtered['gender_numeric'].value_counts().sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(f\"N = {N}\")\n",
    "# Step 2: Age stats\n",
    "age_mean = demo_filtered['age'].mean()\n",
    "age_sd =demo_filtered['age'].std()\n",
    "age_min = demo_filtered['age'].min()\n",
    "age_max = demo_filtered['age'].max()\n",
    "print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "print(\"Gender distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    percent = gender_percent[gender]\n",
    "    print(f\"  Gender {gender}: {count} ({percent}%)\")\n",
    "\n",
    "# Number of responsive participants\n",
    "num_responsive = demo_filtered['responsive'].sum()\n",
    "print(f\"Responsive participants: {num_responsive} out of {N} ({round(100 * num_responsive / N, 1)}%)\")\n",
    "\n",
    "# Income distribution\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "income_mean = round(demo_filtered['income_numeric'].mean(), 2)\n",
    "income_sd = round(demo_filtered['income_numeric'].std(), 2)\n",
    "print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "income_min = demo_filtered['income_numeric'].min()\n",
    "income_max = demo_filtered['income_numeric'].max()\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "print(f\"The household income for Study 1 participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n",
    "\n",
    "\n",
    "# Race distribution (including missing)\n",
    "race_counts = demo_filtered['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "print(\"Race distribution:\")\n",
    "for race, count in race_counts.items():\n",
    "    label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "    percent = race_percent[race]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# College year distribution (categorical)\n",
    "college_counts = demo_filtered['college_year'].value_counts().sort_index()\n",
    "college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "print(\"College year distribution:\")\n",
    "for year, count in college_counts.items():\n",
    "    label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "    percent = college_percent[year]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique 'id's in each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    unique_count = df['id'].nunique()\n",
    "    print(f\"{name}: {unique_count} unique IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 2\n",
    "# Step 1: Merge on 'id' (left join)\n",
    "merged = pd.merge(b2_group_subjective_test, baseline_demo_study2, on='id', how='left')\n",
    "\n",
    "merged.columns\n",
    "# Step 2: Age stats\n",
    "age_mean = merged['age'].mean()\n",
    "age_sd = merged['age'].std()\n",
    "\n",
    "# Step 3: Gender distribution\n",
    "gender_counts = merged['gender_numeric'].value_counts().sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "\n",
    "# Step 4: Responsive distribution\n",
    "responsive_counts = merged['responsive'].value_counts().sort_index()\n",
    "responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "\n",
    "# Output\n",
    "age_min = merged['age'].min()\n",
    "age_max = merged['age'].max()\n",
    "print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "# Gender distribution (including missing)\n",
    "gender_counts = merged['gender_numeric'].value_counts(dropna=False).sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "print(\"Gender distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    label = \"Missing\" if pd.isna(gender) else f\"Gender {gender}\"\n",
    "    percent = gender_percent[gender]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Responsive distribution (including missing)\n",
    "responsive_counts = merged['responsive'].value_counts(dropna=False).sort_index()\n",
    "responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "print(\"Responsive distribution:\")\n",
    "for resp, count in responsive_counts.items():\n",
    "    label = \"Missing\" if pd.isna(resp) else f\"Responsive = {resp}\"\n",
    "    percent = responsive_percent[resp]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Race distribution (including missing)\n",
    "race_counts = merged['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "print(\"Race distribution:\")\n",
    "for race, count in race_counts.items():\n",
    "    label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "    percent = race_percent[race]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# College year distribution (including missing)\n",
    "college_counts = merged['college_year'].value_counts(dropna=False).sort_index()\n",
    "college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "print(\"College year distribution:\")\n",
    "for year, count in college_counts.items():\n",
    "    label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "    percent = college_percent[year]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Income distribution\n",
    "income_median = merged['income_numeric'].median()\n",
    "income_mean = round(merged['income_numeric'].mean(), 2)\n",
    "income_sd = round(merged['income_numeric'].std(), 2)\n",
    "print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "income_min = merged['income_numeric'].min()\n",
    "income_max = merged['income_numeric'].max()\n",
    "income_median = merged['income_numeric'].median()\n",
    "print(f\"The household income for Study 1 participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "def run_rf_train_test(dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5, model_choice_metric='f1', \n",
    "                      res_dir=f\"./results/\", model_type='xgb', test_set=0.3, permutation=False):\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{SEED}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    keys = list(dataframes.keys())\n",
    "\n",
    "    # combine data categories\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {} \n",
    "    best_shap_vals = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "        \n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "    \n",
    "        all_shap_values = []\n",
    "        all_test_data = []\n",
    "        best_overall_score = -np.inf \n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "        best_shap_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps): # i repetitions of train/test\n",
    "\n",
    "            # Prepare train/test split for this i (random & stratified)\n",
    "            X_data, Y_data, X_test, Y_test = prepare_features_and_targets(merged_df.copy(), test_set=test_set, target_var=TARGET_VAR)\n",
    "\n",
    "            # Shuffle labels for permutation tests\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            best_model, best_scores, best_params = random_forest_kfold_grid_search(X_data, Y_data, \n",
    "                                                                                    param_grid, k=k, \n",
    "                                                                                    CV_reps=CV_reps, \n",
    "                                                                                    eval_metric=eval_metrics,\n",
    "                                                                                    model_choice_metric=model_choice_metric,\n",
    "                                                                                    res_dir=res_dir,\n",
    "                                                                                    model_type=model_type,\n",
    "                                                                                    combo=combo)\n",
    "            # Collect metrics\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain the best model on the full training dataset and evaluate on the test set\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test) \n",
    "            shap_values = shap_values[:, :, 1]\n",
    "\n",
    "            # Append SHAP values and test data for later aggregation\n",
    "            all_shap_values.append(shap_values)\n",
    "            all_test_data.append(pd.DataFrame(X_test))\n",
    "\n",
    "            if best_scores[model_choice_metric] > best_overall_score:\n",
    "                best_overall_score = best_scores[model_choice_metric]\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "                best_shap_for_combo = shap_values  # Store SHAP values if needed\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores[model_choice_metric], deepcopy(best_model)))\n",
    "\n",
    "            # Calculate and append metrics for the test set\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep track of the best model based on the model_choice_metric\n",
    "        if combo not in best_models or best_scores[model_choice_metric] > combo_validation_scores[combo][model_choice_metric]['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            \n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "\n",
    "            best_shap_vals[combo] = best_shap_for_combo\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            # Save top 10 models for group_sub combo\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "\n",
    "                # Sort and save top 10 by score\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:100]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top100_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "\n",
    "                # Store back in locals so it's not overwritten each time\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        top2_features = plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo)\n",
    "\n",
    "        plot_pdp_across_runs(\n",
    "            best_model=best_model_for_combo,\n",
    "            res_dir=res_dir,\n",
    "            all_test_data=all_test_data,\n",
    "            interaction_pair=tuple(top2_features)\n",
    "        )\n",
    "\n",
    "        # Calculate mean and 95% CI for validation scores\n",
    "        z = norm.ppf(0.975)  # 95% confidence level\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        # Calculate mean and 95% CI for test scores\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        # For validation scores\n",
    "        df_val = flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        # For test scores\n",
    "        df_test = flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "        \n",
    "    return res_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analyses\n",
    "\n",
    "In this section, all feature categories and their combinations are used to train Random Forest models and evaluate them in a nested CV.\n",
    "\n",
    "Results of every run are stored, alongside SHAP and PDP plots using the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'demo': b5_demographic_response,\n",
    "    # 'alc_self': b1_alcohol_self_response,\n",
    "    # 'psych': b6_psychometric_response,\n",
    "    # 'group_sub': b2_group_subjective_response,\n",
    "    # # 'group_socio': b3_group_sociometric_response,\n",
    "    # 'brain': b4_brain_response,\n",
    "    # 'group_selfreport': b7_objective_group_drinking_response\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model parameter grid to use in k-fold grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [2, 3, 5]\n",
    "}\n",
    "\n",
    "eval_metrics = ['auc', 'f1', 'accuracy', 'specificity', 'sensitivity', 'PPV', 'NPV', 'MCC', 'balancedAcc', 'pr_auc', 'tn', 'fn', 'tp', 'fp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = run_rf_train_test(\n",
    "    dataframes=dataframes,\n",
    "    param_grid=param_grid,\n",
    "    eval_metrics=eval_metrics,\n",
    "    outer_reps=100, # reduce for faster run --> this affects the results\n",
    "    k=3,\n",
    "    CV_reps=5,\n",
    "    model_choice_metric='auc',\n",
    "    res_dir=\"../results/\",\n",
    "    model_type='rf',\n",
    "    test_set=0.3,\n",
    "    permutation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_dir = \"../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=os.path.join(res_dir, 'permutation_test'),\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As detailed in the accompanying paper, test data is available for the feature category `group subjective (GRP)`. This independent follow-up data sample is used as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse_og'\n",
    "loaded_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "# Out-of sample testing, without resampling\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "scores, best_params = test_oos(b2_group_subjective_test, res_dir, loaded_model, None, plot=True, target_var=TARGET_VAR)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of sample testing with resampling to the positive rate of the training data\n",
    "if SEED:\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "res_dir = '../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse_og'\n",
    "summary_df, all_scores_df = evaluate_top_models(\n",
    "    res_dir=res_dir,\n",
    "    test_df=b2_group_subjective_test,\n",
    "    top_n=1,\n",
    "    n_iterations=100,\n",
    "    desired_positive_rate=0.22,\n",
    "    plot=False,\n",
    "    target_var=TARGET_VAR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "model_dir = \"../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse/permutation_test/1754476983_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permTrue/top100_group_sub_models\"\n",
    "model_files = sorted([f for f in os.listdir(model_dir) if f.endswith(\".joblib\")])\n",
    "\n",
    "# Storage for all permutation scores\n",
    "perm_model_scores = []\n",
    "\n",
    "# Evaluate each model on the real test set\n",
    "for model_file in tqdm(model_files):\n",
    "    model_path = os.path.join(model_dir, model_file)\n",
    "    loaded_model = joblib.load(model_path)\n",
    "    \n",
    "    # Run model on the original test set\n",
    "    scores, _ = test_oos(b2_group_subjective_test, model_dir, loaded_model, None, plot=False)\n",
    "    \n",
    "    # Track scores\n",
    "    scores['model_file'] = model_file\n",
    "    perm_model_scores.append(scores)\n",
    "\n",
    "# Create DataFrame of permutation scores\n",
    "perm_model_df = pd.DataFrame(perm_model_scores)\n",
    "\n",
    "# Compare each metric in summary_df to the permutation distribution\n",
    "comparison_results = {}\n",
    "for metric in summary_df.index:\n",
    "    if metric not in perm_model_df.columns:\n",
    "        continue\n",
    "    threshold = summary_df.loc[metric, 'Mean']\n",
    "    count = (perm_model_df[metric] >= threshold).sum()\n",
    "    comparison_results[metric] = {\n",
    "        'NumPermutations >= True': count,\n",
    "        'Proportion': count / len(perm_model_df)\n",
    "    }\n",
    "\n",
    "# Format as DataFrame\n",
    "perm_test_summary = pd.DataFrame(comparison_results).T\n",
    "print(perm_test_summary.sort_values(\"Proportion\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "n_perm = len(perm_model_df)\n",
    "\n",
    "records, pvals = [], []\n",
    "for metric in summary_df.index:\n",
    "    if metric not in perm_model_df.columns:\n",
    "        continue\n",
    "    thr = summary_df.loc[metric, 'Mean']\n",
    "    count = int((perm_model_df[metric] >= thr).sum())\n",
    "    prop = count / n_perm\n",
    "    p = (count + 1) / (n_perm + 1)  # permutation p with +1 correction\n",
    "\n",
    "    records.append({\n",
    "        'Metric': metric,\n",
    "        'Threshold': thr,\n",
    "        'NumPermutations >= Threshold': count,\n",
    "        'Proportion': prop,\n",
    "        'p_value': p\n",
    "    })\n",
    "    pvals.append(p)\n",
    "\n",
    "# FDR correction across metrics\n",
    "_, qvals, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Attach corrected p values\n",
    "for rec, q in zip(records, qvals):\n",
    "    rec['p_value_fdr_bh'] = q\n",
    "\n",
    "perm_test_summary = pd.DataFrame(records).set_index('Metric').sort_values('p_value_fdr_bh')\n",
    "print(perm_test_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CV with different values for k and different train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sensitivity \n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.4,\n",
    "#     permutation=False\n",
    "# )\n",
    "\n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=5,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
