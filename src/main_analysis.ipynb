{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Intervention Responsiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook holds the code for the main analysis of the paper \"Peer Perceptions Emerge as Key Predictors in Multimodal Models of Digital Alcohol Intervention Effectiveness\" submitted to npj Digital Medicine.\n",
    "\n",
    "**Data modalities for the main analysis are:**\n",
    "- `b1_alcohol_self`: Self-reported individual alcohol use and related behaviors.\n",
    "- `b2_group_subjective`: Participant perceptions of their social group’s drinking norms, attitudes, and approval. How much a participant perceives their peers to drink.\n",
    "- `b3_group_sociometric`: Social network–derived measures of group structure and connections.\n",
    "- `b4_brain`: Preprocessed MRI-derived measures of brain activity and connectivity in alcohol-related tasks.\n",
    "- `b5_demographic`: Basic participant characteristics such as age, gender, and income.\n",
    "- `b6_psychometric`: Standardized questionnaire-based measures of psychological traits and states.\n",
    "\n",
    "**Additional analysis:**\n",
    "- `b7_objective_group_drinking`: Aggregated group-level drinking data (how much a participant's peers actually drink).\n",
    "(Analyses with this dataframe are commented - they are left in the code so as to make the processing of this data transparent. However, this analysis was added after the main analysis to compare the predictive utility of objective group drinking compared to group perceptions. This is reported in the supplements of the above paper.)\n",
    "\n",
    "**External test set:**\n",
    "- `b2_group_subjective_study2`: Participant perceptions of their social group’s drinking norms, attitudes, and approval in an independent sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from itertools import combinations, chain\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import spearmanr\n",
    "from pygam import LogisticGAM, s, f\n",
    "\n",
    "# Visualization\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "# Serialization\n",
    "import joblib\n",
    "\n",
    "# Custom\n",
    "from pre_processing import *\n",
    "from training import *\n",
    "from plotting import *\n",
    "from testing import *\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed to replicate the exact results from the paper. Set to `None` to run without random seed (note that this can lead to small deviations in outcome metrics due to randomness in the CV and test process).\n",
    "\n",
    "Please note that the social network data (`b3_group_sociometric`) cannot be publicly provided due to privacy concerns. Running the script without this feature domain may cause some deviations in results from those reported in the main manuscript. Use and processing of this dataframe were left in the notebook but commented so that the processing of this data is still transparent. However, results are qualitatively reproducible and the code shows our procedure in processing and analysing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 321\n",
    "# SEED = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED:\n",
    "    np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define threshold for responsiveness\n",
    "\n",
    "Indicate change threshold that qualifies a participant as responsive vs non-responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE RESPONSIVENESS\n",
    "# avg reduction in drinking occasions between active and control weeks\n",
    "def_response_drink_occasions = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../results\"\n",
    "\n",
    "data_study1 = pd.read_csv('../data/intervention_time/osf_study1.csv')\n",
    "data_study2 = pd.read_csv('../data/intervention_time/osf_study2.csv')\n",
    "\n",
    "# Study 1 baseline data (train/val input)\n",
    "b1_alcohol_self = pd.read_csv('../data/baseline/alcoholself_bucket280225.csv', index_col=0)\n",
    "b2_group_subjective = pd.read_csv('../data/baseline/subjective_grouperceptions_280225.csv', index_col=0)\n",
    "b3_group_sociometric = pd.read_csv('../data/baseline/data_social.csv')\n",
    "b4_brain = pd.read_csv('../data/baseline/brain_bucket_280225.csv', index_col=0)\n",
    "b5_demographic = pd.read_csv('../data/baseline/demographic_bucket280225.csv', index_col=0)\n",
    "b6_psychometric = pd.read_csv('../data/baseline/psychometrics_bucket280225.csv', index_col=0)\n",
    "\n",
    "# # Added analysis - To evaluate performance of objective drinking metrics\n",
    "# b7_objective_group_drinking = pd.read_csv('../data/added_analysis/social_group_drinking.csv', index_col=0)\n",
    "\n",
    "# Study 2 peer perception data (test input)\n",
    "b2_group_subjective_study2 = pd.read_csv('/Users/fmagdalena/Documents/GitHub/shine-network-analysis/SHINE/final_buckets/subjective_grouperceptions_test.csv')\n",
    "baseline_demo_study2 = pd.read_csv('/Users/fmagdalena/Documents/GitHub/SHINE-responsiveness-analysis/data/baseline/demo_study2_full.csv')\n",
    "baseline_demo_all = pd.read_csv('/Users/fmagdalena/Documents/GitHub/SHINE-responsiveness-analysis/data/baseline/00_baseline_survey.csv')\n",
    "baseline_alc_self_all = pd.read_csv('/Users/fmagdalena/Documents/GitHub/SHINE-responsiveness-analysis/data/baseline/00_baseline_survey.csv')\n",
    "\n",
    "# Study 1 & 2 drinking/responsiveness data (output -> prediction target)\n",
    "if def_response_drink_occasions == -1:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -0.5:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-0_5.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -2:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-2.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == 'who':\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_who_rdl.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -0.9:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-0.9.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -1.5:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-1.5.csv', index_col=0).reset_index()\n",
    "elif def_response_drink_occasions == -0.1:\n",
    "    responsive_study1 = pd.read_csv('../data/intervention_time/responsiveness_study1_-0.1.csv', index_col=0).reset_index()\n",
    "\n",
    "responsive_study2 = pd.read_csv('../data/intervention_time/responsiveness_study2.csv', index_col=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_study1_control = data_study1[data_study1.condition == 'control']\n",
    "data_study2_control = data_study2[data_study2.condition == 'control']\n",
    "\n",
    "len(data_study1_control['id'].unique())\n",
    "len(data_study2_control['id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates within each DataFrame\n",
    "duplicates_study1 = responsive_study1['id'].duplicated().any()\n",
    "duplicates_study2 = responsive_study2['id'].duplicated().any()\n",
    "\n",
    "## TEST independently for one condition [mindful | perspective]\n",
    "# responsive_study1 = responsive_study1[responsive_study1.condition == 'mindful']\n",
    "# responsive_study2 = responsive_study2[responsive_study2.condition == 'mindful']\n",
    "## ---\n",
    "# responsive_study1 = responsive_study1[responsive_study1.condition == 'perspective']\n",
    "# responsive_study2 = responsive_study2[responsive_study2.condition == 'perspective']\n",
    "\n",
    "print(f\"Study 1 has duplicates: {duplicates_study1}\")\n",
    "print(f\"Study 2 has duplicates: {duplicates_study2}\")\n",
    "\n",
    "# Check for overlapping IDs between the two studies\n",
    "ids_study1 = set(responsive_study1['id'])\n",
    "ids_study2 = set(responsive_study2['id'])\n",
    "overlap = ids_study1.intersection(ids_study2)\n",
    "\n",
    "print(f\"Number of overlapping IDs: {len(overlap)}\") # Should be zero\n",
    "if overlap:\n",
    "    print(f\"Overlapping IDs: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_VARS = [\n",
    "    'group', 'condition', 'active',\n",
    "    'control', 'difference_drinks_occasions']\n",
    "\n",
    "responsive_study1.drop(columns=EXCLUDE_VARS, inplace=True, errors='ignore')\n",
    "responsive_study2.drop(columns=EXCLUDE_VARS, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsive_study2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Baseline and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets -> Study 1\n",
    "b1_alcohol_self_response = pd.merge(b1_alcohol_self, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response = pd.merge(b2_group_subjective, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response_old = pd.merge(responsive_study1, responsive_study1, on='id', how='inner')\n",
    "b3_group_sociometric_response = pd.merge(b3_group_sociometric, responsive_study1, on='id', how='inner')\n",
    "b4_brain_response = pd.merge(b4_brain, responsive_study1, on='id', how='inner')\n",
    "b5_demographic_response = pd.merge(b5_demographic, responsive_study1, on='id', how='inner')\n",
    "b6_psychometric_response = pd.merge(b6_psychometric, responsive_study1, on='id', how='inner')\n",
    "\n",
    "# b7_objective_group_drinking_response = pd.merge(b7_objective_group_drinking, responsive_study1, on='id', how='inner')\n",
    "\n",
    "print(f'Total IDs Study 1: {len(b1_alcohol_self_response)}')\n",
    "print(f'Responsive IDs Study 1: {b1_alcohol_self_response[b1_alcohol_self_response[\"responsive\"] == 1][\"id\"].nunique()}')\n",
    "print('----------')\n",
    "# Testing dataset -> Study 2\n",
    "b2_group_subjective_test = pd.merge(b2_group_subjective_study2, responsive_study2, on='id', how='inner')\n",
    "print(f'Total IDs Study 2: {len(b2_group_subjective_test)}')\n",
    "print(f'Responsive IDs Study 2: {b2_group_subjective_test[b2_group_subjective_test[\"responsive\"] == 1][\"id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    # 'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    # 'group_socio': b3_group_sociometric_response,\n",
    "    # 'brain': b4_brain_response,\n",
    "    # 'demo': b5_demographic_response,\n",
    "    # 'psych': b6_psychometric_response\n",
    "}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Missing values in '{key}':\")\n",
    "    print(df.isna().sum())\n",
    "    print()  # for spacing between outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find highly correlated features within buckets\n",
    "Find redundancy in features if they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    # 'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'responsive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8, target_var=TARGET_VAR)\n",
    "\n",
    "# Display results\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice is made manually \n",
    "\n",
    "dataframes['brain'].drop(columns=['reward', 'ROI_alc_react_v_rest_neurosynth_cogcontrol', 'ROI_alc_react_v_rest_neurosynth_craving', \\\n",
    "                                  'ROI_alc_react_v_rest_neurosynth_emoreg'], inplace=True)\n",
    "\n",
    "# dataframes['group_socio'].drop(columns=['leaders_deg_in', 'goToBad_deg_in'], inplace=True)\n",
    "\n",
    "dataframes['psych'].drop(columns=['ACS_focus', 'DERS_strategies', 'BIS_attention_total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all within-category correlations are gone\n",
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8)\n",
    "\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of remaining features per category\n",
    "{key: df.shape[1] for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Count initial number of unique IDs in each DataFrame\n",
    "print(\"Initial ID counts per dataframe:\")\n",
    "initial_counts = {name: df['id'].nunique() for name, df in dataframes.items()}\n",
    "for name, count in initial_counts.items():\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# 2) Identify and remove IDs with >10 missing values across all dataframes\n",
    "# Count missing values per ID across all dataframes\n",
    "missing_counts = Counter()\n",
    "for df in dataframes.values():\n",
    "    id_missing = df.set_index('id').isnull().sum(axis=1)\n",
    "    for idx, val in id_missing.items():\n",
    "        if val > 0:\n",
    "            missing_counts[idx] += val\n",
    "\n",
    "# Get IDs with >10 missing values in total\n",
    "bad_ids = {id_ for id_, miss_count in missing_counts.items() if miss_count > 10}\n",
    "\n",
    "# Drop those IDs from all dataframes\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][~dataframes[name]['id'].isin(bad_ids)]\n",
    "\n",
    "print(\"\\nID counts after removing IDs with >10 total missing features:\")\n",
    "post_clean_counts = {name: df['id'].nunique() for name, df in dataframes.items()}\n",
    "for name, count in post_clean_counts.items():\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# 3) Intersect IDs: keep only IDs present in all dataframes\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n",
    "\n",
    "# 4) Final N\n",
    "final_N = len(common_ids)\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n",
    "print(f\"\\nFinal number of participants present in all dataframes: N = {final_N}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Study 2\n",
    "b2_group_subjective_test\n",
    "\n",
    "# 1) Count initial number of unique IDs in each DataFrame\n",
    "print(\"Initial ID counts study 2:\")\n",
    "initial_counts = b2_group_subjective_test['id'].nunique()\n",
    "print(f\"  group_subjective_test: {initial_counts}\")\n",
    "\n",
    "# Remove rows with more than 1 missing feature\n",
    "b2_group_subjective_test = b2_group_subjective_test[b2_group_subjective_test.isnull().sum(axis=1) <= 1]\n",
    "\n",
    "# Check how many IDs remain\n",
    "remaining_ids = b2_group_subjective_test['id'].nunique()\n",
    "print(f\"  group_subjective_test (after removing >1 missing): {remaining_ids}\")\n",
    "print(f\"Responsive: {b2_group_subjective_test['responsive'].sum()} out of {b2_group_subjective_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Characteristics Study 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique 'id's in each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    unique_count = df['id'].nunique()\n",
    "    print(f\"{name}: {unique_count} unique IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get common IDs\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "\n",
    "# Step 2: Filter each DataFrame to keep only rows with common IDs\n",
    "for name in dataframes:\n",
    "    dataframes[name] = dataframes[name][dataframes[name]['id'].isin(common_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get common IDs\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "\n",
    "# Step 2: Filter demographics\n",
    "demo_filtered = dataframes['demo'][dataframes['demo']['id'].isin(common_ids)]\n",
    "demo_filtered[\"id\"].to_csv(\"../data/study_1_ids.csv\")\n",
    "\n",
    "# Step 3: Compute summaries\n",
    "N = demo_filtered.shape[0]\n",
    "age_mean = round(demo_filtered['age'].mean(), 2)\n",
    "age_sd = round(demo_filtered['age'].std(), 2)\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = demo_filtered['gender_numeric'].value_counts().sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(f\"N = {N}\")\n",
    "# Step 2: Age stats\n",
    "age_mean = demo_filtered['age'].mean()\n",
    "age_sd =demo_filtered['age'].std()\n",
    "age_min = demo_filtered['age'].min()\n",
    "age_max = demo_filtered['age'].max()\n",
    "print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "print(\"Gender distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    percent = gender_percent[gender]\n",
    "    print(f\"  Gender {gender}: {count} ({percent}%)\")\n",
    "\n",
    "# Number of responsive participants\n",
    "num_responsive = demo_filtered['responsive'].sum()\n",
    "print(f\"Responsive participants: {num_responsive} out of {N} ({round(100 * num_responsive / N, 1)}%)\")\n",
    "\n",
    "# Income distribution\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "income_mean = round(demo_filtered['income_numeric'].mean(), 2)\n",
    "income_sd = round(demo_filtered['income_numeric'].std(), 2)\n",
    "print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "income_min = demo_filtered['income_numeric'].min()\n",
    "income_max = demo_filtered['income_numeric'].max()\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "print(f\"The household income for Study 1 participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n",
    "\n",
    "\n",
    "# Race distribution (including missing)\n",
    "race_counts = demo_filtered['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "print(\"Race distribution:\")\n",
    "for race, count in race_counts.items():\n",
    "    label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "    percent = race_percent[race]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# College year distribution (categorical)\n",
    "college_counts = demo_filtered['college_year'].value_counts().sort_index()\n",
    "college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "print(\"College year distribution:\")\n",
    "for year, count in college_counts.items():\n",
    "    label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "    percent = college_percent[year]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique 'id's in each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    unique_count = df['id'].nunique()\n",
    "    print(f\"{name}: {unique_count} unique IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/added_analysis/raw_response.csv\")\n",
    "\n",
    "# Keep only IDs that exist in alc_self\n",
    "df = df[df['id'].isin(dataframes['alc_self']['id'])]\n",
    "\n",
    "# Color mapping\n",
    "palette = {\n",
    "    'perspective': '#006400',  # dark green\n",
    "    'mindful': '#800080'       # purple\n",
    "}\n",
    "\n",
    "# Plot\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "ax = sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"condition\",\n",
    "    y=\"difference_occasions\",\n",
    "    hue=\"condition\",\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    "    size=6,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Add horizontal lines at thresholds\n",
    "for thresh in [-0.5, -1.0, -2.0]:\n",
    "    plt.axhline(y=thresh, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.text(1.05, thresh+0.05, f'{thresh}', color='grey', fontsize=9)\n",
    "\n",
    "# Add 0 reference line\n",
    "plt.axhline(y=0, color='black', linewidth=1)\n",
    "\n",
    "# Labels\n",
    "plt.title('Difference in Drinking Occasions (Active − Control)', fontsize=14)\n",
    "plt.ylabel('Difference in Drinking Occasions')\n",
    "plt.xlabel('')\n",
    "plt.legend([], [], frameon=False)  # remove redundant legend since color = x variable\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load and filter data ---\n",
    "df = pd.read_csv(\"../data/added_analysis/raw_response.csv\")\n",
    "df = df[df['id'].isin(dataframes['alc_self']['id'])]\n",
    "\n",
    "# --- Color mapping ---\n",
    "palette = {\n",
    "    'perspective': '#006400',  # dark green\n",
    "    'mindful': '#800080'       # purple\n",
    "}\n",
    "\n",
    "# --- Create subplots ---\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# --- (1) Pooled box+swarm across both conditions ---\n",
    "sns.boxplot(\n",
    "    data=df, y=\"difference_occasions\", ax=axes[0],\n",
    "    color='lightgray', width=0.4, showfliers=False\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=df, y=\"difference_occasions\", color='black',\n",
    "    alpha=0.7, size=5, ax=axes[0]\n",
    ")\n",
    "axes[0].axhline(y=0, color='black', linewidth=1)\n",
    "for thresh in [-0.5, -1.0, -2.0]:\n",
    "    axes[0].axhline(y=thresh, color='grey', linestyle='--', linewidth=1)\n",
    "axes[0].set_title('Difference in drinking occasions (active − control) (both strategies)', fontsize=14)\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Δ Control − Active \\n (drinking occasions/week)', fontsize=13)\n",
    "\n",
    "# --- (2) Your original per-condition swarm plot ---\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"condition\",\n",
    "    y=\"difference_occasions\",\n",
    "    hue=\"condition\",\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    "    size=6,\n",
    "    alpha=0.9,\n",
    "    ax=axes[1]\n",
    ")\n",
    "\n",
    "for thresh in [-0.5, -1.0, -2.0]:\n",
    "    axes[1].axhline(y=thresh, color='grey', linestyle='--', linewidth=1)\n",
    "    axes[1].text(1.05, thresh + 0.05, f'{thresh}', color='grey', fontsize=10)\n",
    "\n",
    "axes[1].axhline(y=0, color='black', linewidth=1)\n",
    "axes[1].set_title('Difference in drinking occasions by strategy', fontsize=14)\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].legend([], [], frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are more adherent people more likely to respond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_main = pd.read_csv(\"../data/intervention_time/responsiveness_study1.csv\")\n",
    "df_adherence_1 = pd.read_csv(\"../data/intervention_time/osf_study1.csv\")[['id', 'alc_responses']].drop_duplicates('id')\n",
    "\n",
    "# Merge on id\n",
    "merged_df = df_main.merge(df_adherence_1, on='id', how='inner')\n",
    "\n",
    "# Drop rows with missing adherence values (if any)\n",
    "merged_df = merged_df.dropna(subset=['alc_responses'])\n",
    "merged_df['alc_responses'] = (merged_df['alc_responses'] / 56) * 100\n",
    "merged_df = merged_df[merged_df['id'].isin(dataframes['alc_self']['id'])]\n",
    "\n",
    "# --- Stats test ---\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Split by responsiveness\n",
    "group0 = merged_df.loc[merged_df['responsive'] == 0, 'alc_responses']\n",
    "group1 = merged_df.loc[merged_df['responsive'] == 1, 'alc_responses']\n",
    "\n",
    "# Mann–Whitney U test (two-sided)\n",
    "stat, pval = mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "print(f\"Mann–Whitney U = {stat:.2f}, p = {pval:.3f}\")\n",
    "\n",
    "# --- Boxplot ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=merged_df, x='responsive', y='alc_responses', palette=['gray', 'green'])\n",
    "sns.stripplot(data=merged_df, x='responsive', y='alc_responses', color='black', alpha=0.3)\n",
    "plt.xlabel('Responsive (0 = No, 1 = Yes)')\n",
    "plt.ylabel('% of prompts completed')\n",
    "plt.title(f'Study 1: Adherence by responsiveness status (Mann-Whitney U test, p = {pval:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = (\n",
    "    merged_df.groupby('responsive')['alc_responses']\n",
    "    .agg(['min', 'max', 'median', 'std', 'count'])\n",
    "    .rename(index={0: 'Non-responders', 1: 'Responders'})\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = merged_df['alc_responses']\n",
    "\n",
    "overall_stats = pd.Series({\n",
    "    'mean': overall.mean(),\n",
    "    'std': overall.std(),\n",
    "    'min': overall.min(),\n",
    "    'max': overall.max(),\n",
    "    'median': overall.median(),\n",
    "    'count': overall.count()\n",
    "}).round(2)\n",
    "\n",
    "print(\"=== Overall adherence (all participants) ===\")\n",
    "print(overall_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_main = pd.read_csv(\"../data/intervention_time/responsiveness_study2.csv\")\n",
    "df_adherence_2 = pd.read_csv(\"../data/intervention_time/osf_study2.csv\")[['id', 'alc_responses']].drop_duplicates('id')\n",
    "\n",
    "# Merge on id\n",
    "merged_df = df_main.merge(df_adherence_2, on='id', how='inner')\n",
    "\n",
    "# Drop rows with missing adherence values (if any)\n",
    "merged_df = merged_df.dropna(subset=['alc_responses'])\n",
    "# Convert raw counts to percentage of prompts completed\n",
    "merged_df['alc_responses'] = (merged_df['alc_responses'] / 56) * 100\n",
    "merged_df = merged_df[merged_df['id'].isin(responsive_study2['id'])]\n",
    "\n",
    "# --- Stats test ---\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Split by responsiveness\n",
    "group0 = merged_df.loc[merged_df['responsive'] == 0, 'alc_responses']\n",
    "group1 = merged_df.loc[merged_df['responsive'] == 1, 'alc_responses']\n",
    "\n",
    "# Mann–Whitney U test (two-sided)\n",
    "stat, pval = mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "print(f\"Mann–Whitney U = {stat:.2f}, p = {pval:.3f}\")\n",
    "\n",
    "# --- Boxplot ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=merged_df, x='responsive', y='alc_responses', palette=['gray', 'green'])\n",
    "sns.stripplot(data=merged_df, x='responsive', y='alc_responses', color='black', alpha=0.3)\n",
    "plt.xlabel('Responsive (0 = No, 1 = Yes)')\n",
    "plt.ylabel('% of prompts completed')\n",
    "plt.title(f'Study 2: Adherence by responsiveness status (Mann-Whitney U test, p = {pval:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = (\n",
    "    merged_df.groupby('responsive')['alc_responses']\n",
    "    .agg(['min', 'max', 'median', 'std', 'count'])\n",
    "    .rename(index={0: 'Non-responders', 1: 'Responders'})\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = merged_df['alc_responses']\n",
    "\n",
    "overall_stats = pd.Series({\n",
    "    'mean': overall.mean(),\n",
    "    'std': overall.std(),\n",
    "    'min': overall.min(),\n",
    "    'max': overall.max(),\n",
    "    'median': overall.median(),\n",
    "    'count': overall.count()\n",
    "}).round(2)\n",
    "\n",
    "print(\"=== Overall adherence (all participants) ===\")\n",
    "print(overall_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there demographic differences between responders and non-responders?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get common IDs\n",
    "common_ids = set.intersection(*[set(df['id']) for df in dataframes.values()])\n",
    "\n",
    "# Step 2: Filter demographics\n",
    "demo_filtered = dataframes['demo'][dataframes['demo']['id'].isin(common_ids)]\n",
    "\n",
    "# Step 3: Compute summaries\n",
    "N = demo_filtered.shape[0]\n",
    "age_mean = round(demo_filtered['age'].mean(), 2)\n",
    "age_sd = round(demo_filtered['age'].std(), 2)\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = demo_filtered['gender_numeric'].value_counts().sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(f\"N = {N}\")\n",
    "# Step 2: Age stats\n",
    "age_mean = demo_filtered['age'].mean()\n",
    "age_sd =demo_filtered['age'].std()\n",
    "age_min = demo_filtered['age'].min()\n",
    "age_max = demo_filtered['age'].max()\n",
    "print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "print(\"Gender distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    percent = gender_percent[gender]\n",
    "    print(f\"  Gender {gender}: {count} ({percent}%)\")\n",
    "\n",
    "# Number of responsive participants\n",
    "num_responsive = demo_filtered['responsive'].sum()\n",
    "print(f\"Responsive participants: {num_responsive} out of {N} ({round(100 * num_responsive / N, 1)}%)\")\n",
    "\n",
    "# Income distribution\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "income_mean = round(demo_filtered['income_numeric'].mean(), 2)\n",
    "income_sd = round(demo_filtered['income_numeric'].std(), 2)\n",
    "print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "income_min = demo_filtered['income_numeric'].min()\n",
    "income_max = demo_filtered['income_numeric'].max()\n",
    "income_median = demo_filtered['income_numeric'].median()\n",
    "print(f\"The household income for Study 1 participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n",
    "\n",
    "\n",
    "# Race distribution (including missing)\n",
    "race_counts = demo_filtered['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "print(\"Race distribution:\")\n",
    "for race, count in race_counts.items():\n",
    "    label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "    percent = race_percent[race]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# College year distribution (categorical)\n",
    "college_counts = demo_filtered['college_year'].value_counts().sort_index()\n",
    "college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "print(\"College year distribution:\")\n",
    "for year, count in college_counts.items():\n",
    "    label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "    percent = college_percent[year]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\n",
    "    \"Responsive\": demo_filtered[demo_filtered['responsive'] == 1],\n",
    "    \"Non-responsive\": demo_filtered[demo_filtered['responsive'] == 0]\n",
    "}\n",
    "\n",
    "for name, df in groups.items():\n",
    "    print(f\"\\n===== {name} participants =====\")\n",
    "    N = df.shape[0]\n",
    "    print(f\"N = {N}\")\n",
    "\n",
    "    # Age\n",
    "    age_mean = df['age'].mean()\n",
    "    age_sd = df['age'].std()\n",
    "    age_min = df['age'].min()\n",
    "    age_max = df['age'].max()\n",
    "    print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "\n",
    "    # Gender\n",
    "    gender_counts = df['gender_numeric'].value_counts().sort_index()\n",
    "    gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "    print(\"Gender distribution:\")\n",
    "    for gender, count in gender_counts.items():\n",
    "        percent = gender_percent[gender]\n",
    "        print(f\"  Gender {gender}: {count} ({percent}%)\")\n",
    "\n",
    "    # Income\n",
    "    income_mean = df['income_numeric'].mean()\n",
    "    income_sd = df['income_numeric'].std()\n",
    "    income_min = df['income_numeric'].min()\n",
    "    income_max = df['income_numeric'].max()\n",
    "    income_median = df['income_numeric'].median()\n",
    "    print(f\"Income: Median = {income_median}, Mean = {round(income_mean, 2)}, SD = {round(income_sd, 2)}, \"\n",
    "          f\"Range = [{income_min}, {income_max}]\")\n",
    "\n",
    "    # Race\n",
    "    race_counts = df['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "    race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "    print(\"Race distribution:\")\n",
    "    for race, count in race_counts.items():\n",
    "        label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "        percent = race_percent[race]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "    # College year\n",
    "    college_counts = df['college_year'].value_counts().sort_index()\n",
    "    college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "    print(\"College year distribution:\")\n",
    "    for year, count in college_counts.items():\n",
    "        label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "        percent = college_percent[year]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline drinking levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_alcohol_self_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "baseline_alc_self = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(demo_filtered['id'])\n",
    "]\n",
    "\n",
    "baseline_alc_self = baseline_alc_self.merge(b1_alcohol_self_response, left_on='pID', right_on='id', how='inner')\n",
    "\n",
    "# Pool responders + non-responders\n",
    "baseline_intervention = baseline_alc_self[\n",
    "    baseline_alc_self['responsive'].isin([0, 1])\n",
    "]\n",
    "\n",
    "# Clean invalid values\n",
    "baseline_intervention = baseline_intervention[\n",
    "    (baseline_intervention['freq_self'] >= 0) &\n",
    "    (baseline_intervention['amount_self'] >= 0)\n",
    "]\n",
    "control_unique = (\n",
    "    data_study2_control[['id', 'alc_responses']]\n",
    "    .drop_duplicates('id')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "control_ids = control_unique[['id']].drop_duplicates('id')\n",
    "\n",
    "# Baseline rows for controls\n",
    "baseline_controls = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(control_ids['id'])\n",
    "].copy()\n",
    "\n",
    "baseline_controls = baseline_controls[\n",
    "    (baseline_controls['freq_self'] >= 0) &\n",
    "    (baseline_controls['amount_self'] >= 0)\n",
    "]\n",
    "\n",
    "# Extract numeric series\n",
    "ctrl_freq = baseline_controls['freq_self'].dropna()\n",
    "int_freq = baseline_intervention['freq_self'].dropna()\n",
    "\n",
    "ctrl_amount = baseline_controls['amount_self'].dropna()\n",
    "int_amount = baseline_intervention['amount_self'].dropna()\n",
    "\n",
    "# Welch t-tests (unequal variances)\n",
    "t_freq, p_t_freq = ttest_ind(ctrl_freq, int_freq, equal_var=False)\n",
    "t_amount, p_t_amount = ttest_ind(ctrl_amount, int_amount, equal_var=False)\n",
    "\n",
    "print(\"=== Welch t-test: Control vs Intervention (pooled) ===\")\n",
    "print(f\"freq_self: t = {t_freq:.2f}, p = {p_t_freq:.3f}\")\n",
    "print(f\"amount_self: t = {t_amount:.2f}, p = {p_t_amount:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "# 1. Filter AUQ dataframe to only IDs present in demo\n",
    "auq_filtered = b1_alcohol_self_response[\n",
    "    b1_alcohol_self_response['id'].isin(demo_filtered['id'])\n",
    "].copy()\n",
    "\n",
    "# 2. Split by responder status\n",
    "responders = auq_filtered[auq_filtered['responsive'] == 1].copy()\n",
    "non_responders = auq_filtered[auq_filtered['responsive'] == 0].copy()\n",
    "\n",
    "# 3. Ensure numeric\n",
    "cols = ['AUQ_drink_frequency', 'AUQ_drink_amount']\n",
    "for c in cols:\n",
    "    responders[c] = pd.to_numeric(responders[c], errors='coerce')\n",
    "    non_responders[c] = pd.to_numeric(non_responders[c], errors='coerce')\n",
    "\n",
    "# 4. Run tests\n",
    "for col in cols:\n",
    "    x = responders[col].dropna()\n",
    "    y = non_responders[col].dropna()\n",
    "    \n",
    "    print(f\"\\n=== {col} ===\")\n",
    "    print(\"n_responder:\", len(x), \"n_nonresp:\", len(y))\n",
    "\n",
    "    # Welch t-test (unequal variances)\n",
    "    t_stat, p_t = ttest_ind(x, y, equal_var=False)\n",
    "    print(f\"Welch t-test: t = {t_stat:.2f}, p = {p_t:.3f}\")\n",
    "\n",
    "    # Mann-Whitney U\n",
    "    u_stat, p_u = mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "    print(f\"Mann-Whitney U: U = {u_stat:.2f}, p = {p_u:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "palette = {'0': 'lightgray', '1': 'darkgreen'}\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=False)\n",
    "\n",
    "# --- Frequency (occasions/week) ---\n",
    "sns.boxplot(\n",
    "    data=b1_alcohol_self_response,\n",
    "    x='responsive', y='AUQ_drink_frequency',\n",
    "    ax=axes[0], palette=palette\n",
    ")\n",
    "axes[0].set_title('Self-Reported Drinking Frequency at Baseline (AUQ)')\n",
    "axes[0].set_xlabel('Responsive')\n",
    "axes[0].set_ylabel('Occasions per Week')\n",
    "axes[0].set_xticklabels(['0 (Non-responder)', '1 (Responder)'])\n",
    "\n",
    "# --- Amount (drinks per occasion) ---\n",
    "sns.boxplot(\n",
    "    data=b1_alcohol_self_response,\n",
    "    x='responsive', y='AUQ_drink_amount',\n",
    "    ax=axes[1], palette=palette\n",
    ")\n",
    "axes[1].set_title('Self-Reported Drinking Amount at Baseline (AUQ)')\n",
    "axes[1].set_xlabel('Responsive')\n",
    "axes[1].set_ylabel('Drinks per Occasion')\n",
    "axes[1].set_xticklabels(['0 (Non-responder)', '1 (Responder)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define numeric columns\n",
    "cols = ['AUQ_drink_frequency', 'AUQ_drink_amount']\n",
    "\n",
    "# Run t-tests\n",
    "for col in cols:\n",
    "    t_stat, p_val = ttest_ind(\n",
    "        responders[col].dropna(),\n",
    "        non_responders[col].dropna(),\n",
    "        equal_var=False  # Welch’s t-test (robust to unequal variances)\n",
    "    )\n",
    "    print(f\"{col}: t = {t_stat:.2f}, p = {p_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['AUQ_drink_frequency', 'AUQ_drink_amount']\n",
    "\n",
    "for col in cols:\n",
    "    x = responders[col].dropna().values\n",
    "    y = non_responders[col].dropna().values\n",
    "\n",
    "    # Welch t-test\n",
    "    t_stat, p_val = ttest_ind(x, y, equal_var=False)\n",
    "\n",
    "    n1, n2 = len(x), len(y)\n",
    "    v1, v2 = np.var(x, ddof=1), np.var(y, ddof=1)\n",
    "\n",
    "    df = (v1/n1 + v2/n2)**2 / (\n",
    "        (v1**2) / ((n1**2) * (n1 - 1)) +\n",
    "        (v2**2) / ((n2**2) * (n2 - 1))\n",
    "    )\n",
    "\n",
    "    print(f\"{col}: t({df:.2f}) = {t_stat:.2f}, p = {p_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average drinking on on/off weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "study_1_response= data_study1.merge(demo_filtered, on='id', how='left')\n",
    "study_1_response = study_1_response[study_1_response['id'].isin(demo_filtered['id'])]\n",
    "\n",
    "# Convert drinks_number to numeric just in case\n",
    "data_study1['drinks_number'] = pd.to_numeric(data_study1['drinks_number'], errors='coerce')\n",
    "\n",
    "# Aggregate per participant and week type\n",
    "weekly_stats = (\n",
    "    data_study1\n",
    "    .groupby(['id', 'active_week'])\n",
    "    .agg(\n",
    "        total_drinks=('drinks_number', 'sum'),\n",
    "        occasions_number=('drinks_number', lambda x: (x > 0).sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    weekly_stats\n",
    "    .pivot(index='id', columns='active_week', values=['total_drinks', 'occasions_number'])\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Flatten the column names\n",
    "summary.columns = [f\"{col[0]}_{col[1]}\" for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "summary['avg_drinks_active'] = summary['total_drinks_active'] / summary['occasions_number_active'] \n",
    "summary['avg_drinks_control'] = summary['total_drinks_control'] / summary['occasions_number_control']\n",
    "summary['avg_occasions_active'] = summary['occasions_number_active'] / 2\n",
    "summary['avg_occasions_control'] = summary['occasions_number_control'] / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = summary.merge(\n",
    "    responsive_study1[['id', 'responsive']],\n",
    "    on='id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = merged_final[merged_final['id'].isin(demo_filtered['id'])]\n",
    "merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary stats for responders and non-responders\n",
    "stats = (\n",
    "    merged_final\n",
    "    .groupby('responsive')[[\n",
    "        'avg_drinks_control', 'avg_drinks_active',\n",
    "        'avg_occasions_control', 'avg_occasions_active'\n",
    "    ]]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Flatten multi-level columns for readability\n",
    "stats.columns = [f\"{col[0]}_{col[1]}\" for col in stats.columns]\n",
    "stats = stats.reset_index()\n",
    "\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = merged_final.melt(\n",
    "    id_vars='responsive',\n",
    "    value_vars=[\n",
    "        'avg_drinks_control', 'avg_drinks_active',\n",
    "        'avg_occasions_control', 'avg_occasions_active'\n",
    "    ],\n",
    "    var_name='measure',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Add helper columns\n",
    "plot_df['type'] = plot_df['measure'].apply(lambda x: 'Drinks' if 'drinks' in x else 'Occasions')\n",
    "plot_df['condition'] = plot_df['measure'].apply(lambda x: 'Active' if 'active' in x else 'Control')\n",
    "\n",
    "# Define colors\n",
    "palette = {0: 'lightgray', 1: 'darkgreen'}\n",
    "\n",
    "# Set up plotting\n",
    "sns.set(style='whitegrid')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=False)\n",
    "\n",
    "# --- Drinks plot ---\n",
    "sns.boxplot(\n",
    "    data=plot_df[plot_df['type'] == 'Drinks'],\n",
    "    x='condition', y='value', hue='responsive',\n",
    "    ax=axes[0], palette=palette\n",
    ")\n",
    "axes[0].set_title('Average Drinks per Occasion')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Drinks per Ocaasion')\n",
    "\n",
    "# Manually fix legend colors\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "axes[0].legend(\n",
    "    handles, ['Non-responsive(0)', 'Responsive (1)'],\n",
    "    title='Responsive',\n",
    "    facecolor='white',\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "# --- Occasions plot ---\n",
    "sns.boxplot(\n",
    "    data=plot_df[plot_df['type'] == 'Occasions'],\n",
    "    x='condition', y='value', hue='responsive',\n",
    "    ax=axes[1], palette=palette\n",
    ")\n",
    "axes[1].set_title('Average Drinking Occasions per Week')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('Number of Weekly Occasions')\n",
    "axes[1].legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tests = [\n",
    "    ('avg_drinks_control', 'Control – Drinks per Occasion'),\n",
    "    ('avg_drinks_active', 'Active – Drinks per Occasion'),\n",
    "    ('avg_occasions_control', 'Control – Drinking Occasions per Week'),\n",
    "    ('avg_occasions_active', 'Active – Drinking Occasions per Week')\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for var, label in tests:\n",
    "    # Extract responder groups\n",
    "    res_vals = merged_final.loc[merged_final['responsive'] == 1, var].dropna()\n",
    "    nonres_vals = merged_final.loc[merged_final['responsive'] == 0, var].dropna()\n",
    "\n",
    "    # Welch’s t-test (unequal variances)\n",
    "    t_stat, p_val = ttest_ind(res_vals, nonres_vals, equal_var=False)\n",
    "\n",
    "    results.append({\n",
    "        \"Measure\": label,\n",
    "        \"Responsive Mean\": res_vals.mean(),\n",
    "        \"Non-responsive Mean\": nonres_vals.mean(),\n",
    "        \"t\": t_stat,\n",
    "        \"p\": p_val\n",
    "    })\n",
    "\n",
    "# Create summary table\n",
    "test_df = pd.DataFrame(results).round(3)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Mindful vs Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindful_ids_study1 = study_1_response.loc[study_1_response[\"condition\"] == \"mindful\", \"id\"].unique()\n",
    "perspective_ids_study1 = study_1_response.loc[study_1_response[\"condition\"] == \"perspective\", \"id\"].unique()\n",
    "\n",
    "print(\"Mindful IDs:\", len(mindful_ids_study1))\n",
    "print(\"Perspective IDs:\", len(perspective_ids_study1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adherence by intervention type\n",
    "cond_map = study_1_response[['id', 'condition']].drop_duplicates('id')\n",
    "\n",
    "# --- Merge condition with adherence ---\n",
    "merged_df = df_adherence_1.merge(cond_map, on='id', how='inner')\n",
    "\n",
    "# --- Check ---\n",
    "print(merged_df['condition'].value_counts(dropna=False))\n",
    "\n",
    "# --- Compute adherence stats per condition ---\n",
    "adherence_stats = (\n",
    "    merged_df\n",
    "    .groupby('condition')['alc_responses']\n",
    "    .agg(['mean', 'std', 'min', 'max'])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(adherence_stats/56)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Characteristics Study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 2\n",
    "# Step 1: Merge on 'id' (left join)\n",
    "merged = pd.merge(b2_group_subjective_test, baseline_demo_study2, on='id', how='left')\n",
    "print(len(merged))\n",
    "\n",
    "merged.columns\n",
    "# Step 2: Age stats\n",
    "age_mean = merged['age'].mean()\n",
    "age_sd = merged['age'].std()\n",
    "\n",
    "# Step 3: Gender distribution\n",
    "gender_counts = merged['gender_numeric'].value_counts().sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "\n",
    "# Step 4: Responsive distribution\n",
    "responsive_counts = merged['responsive'].value_counts().sort_index()\n",
    "responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "\n",
    "# Output\n",
    "age_min = merged['age'].min()\n",
    "age_max = merged['age'].max()\n",
    "print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "# Gender distribution (including missing)\n",
    "gender_counts = merged['gender_numeric'].value_counts(dropna=False).sort_index()\n",
    "gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "print(\"Gender distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    label = \"Missing\" if pd.isna(gender) else f\"Gender {gender}\"\n",
    "    percent = gender_percent[gender]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Responsive distribution (including missing)\n",
    "responsive_counts = merged['responsive'].value_counts(dropna=False).sort_index()\n",
    "responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "print(\"Responsive distribution:\")\n",
    "for resp, count in responsive_counts.items():\n",
    "    label = \"Missing\" if pd.isna(resp) else f\"Responsive = {resp}\"\n",
    "    percent = responsive_percent[resp]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Race distribution (including missing)\n",
    "race_counts = merged['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "print(\"Race distribution:\")\n",
    "for race, count in race_counts.items():\n",
    "    label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "    percent = race_percent[race]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# College year distribution (including missing)\n",
    "college_counts = merged['college_year'].value_counts(dropna=False).sort_index()\n",
    "college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "print(\"College year distribution:\")\n",
    "for year, count in college_counts.items():\n",
    "    label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "    percent = college_percent[year]\n",
    "    print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# Income distribution\n",
    "income_median = merged['income_numeric'].median()\n",
    "income_mean = round(merged['income_numeric'].mean(), 2)\n",
    "income_sd = round(merged['income_numeric'].std(), 2)\n",
    "print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "income_min = merged['income_numeric'].min()\n",
    "income_max = merged['income_numeric'].max()\n",
    "income_median = merged['income_numeric'].median()\n",
    "print(f\"The household income for Study 1 participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n",
    "\n",
    "merged[\"id\"].to_csv(\"../data/study_2_ids.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Study 2 controls descriptives ===\n",
    "\n",
    "# # Step 0: unique control IDs\n",
    "# control_ids = data_study2_control[['id']].drop_duplicates('id')\n",
    "\n",
    "# # Step 1: Merge unique control IDs with baseline demographics\n",
    "# merged_control = pd.merge(control_ids, baseline_demo_all, left_on='id', right_on='pID', how='left')\n",
    "# print(\"N unique Study 2 control IDs:\", merged_control['id'].nunique())\n",
    "# print(\"N rows in merged control demo df:\", len(merged_control))\n",
    "\n",
    "# # Step 2: Age stats\n",
    "# age_mean = merged_control['age'].mean()\n",
    "# age_sd = merged_control['age'].std()\n",
    "# age_min = merged_control['age'].min()\n",
    "# age_max = merged_control['age'].max()\n",
    "# print(f\"Age (controls): M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "\n",
    "# # Step 3: Gender distribution (including missing)\n",
    "# gender_counts = merged_control['gender_numeric'].value_counts(dropna=False)\n",
    "# gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "# print(\"Gender distribution (controls):\")\n",
    "# for gender, count in gender_counts.items():\n",
    "#     label = \"Missing\" if pd.isna(gender) else str(gender)\n",
    "#     percent = gender_percent[gender]\n",
    "#     print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# # Step 4: Race distribution (categorical)\n",
    "# race_counts = merged_control['race'].value_counts(dropna=False)\n",
    "# race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "\n",
    "# print(\"Race distribution (controls):\")\n",
    "# for race_value, count in race_counts.items():\n",
    "#     label = \"Missing\" if pd.isna(race_value) else str(race_value)\n",
    "#     percent = race_percent[race_value]\n",
    "#     print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "# # Step 5: Income distribution (controls)\n",
    "# income_mean = round(merged_control['income_num'].mean(), 2)\n",
    "# income_sd = round(merged_control['income_num'].std(), 2)\n",
    "# income_min = merged_control['income_num'].min()\n",
    "# income_max = merged_control['income_num'].max()\n",
    "# income_median = merged_control['income_num'].median()\n",
    "\n",
    "# print(f\"Income (controls): Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "# print(f\"The household income for Study 2 control participants was between \"\n",
    "#       f\"{income_min} and {income_max}, with a median of {income_median}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_study2_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique controls\n",
    "control_ids = data_study2_control[['id']].drop_duplicates('id')\n",
    "\n",
    "# Controls merged with demographics (you already computed this as `merged`)\n",
    "merged_control = pd.merge(control_ids, baseline_demo_all, left_on='id', right_on='pID', how='left')\n",
    "controls = merged_control.copy()   # The merged control dataframe from your code\n",
    "\n",
    "# Non-controls: all in baseline_demo_all not in control_ids\n",
    "non_controls = baseline_demo_all[\n",
    "    ~baseline_demo_all['pID'].isin(control_ids['id'])\n",
    "][['pID', 'gender_numeric', 'race']]\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Build contingency table\n",
    "gender_table = pd.crosstab(\n",
    "    controls['gender_numeric'],\n",
    "    non_controls['gender_numeric']\n",
    ")\n",
    "\n",
    "chi2_g, p_g, dof_g, exp_g = chi2_contingency(gender_table)\n",
    "\n",
    "print(\"=== Chi-square test: Gender (controls vs non-controls) ===\")\n",
    "print(\"Chi2 =\", round(chi2_g, 3), \"p =\", round(p_g, 4))\n",
    "print(gender_table)\n",
    "\n",
    "# Contingency table for race\n",
    "race_table = pd.crosstab(\n",
    "    controls['race'],\n",
    "    non_controls['race']\n",
    ")\n",
    "\n",
    "chi2_r, p_r, dof_r, exp_r = chi2_contingency(race_table)\n",
    "\n",
    "print(\"\\n=== Chi-square test: Race (controls vs non-controls) ===\")\n",
    "print(\"Chi2 =\", round(chi2_r, 3), \"p =\", round(p_r, 4))\n",
    "print(race_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse controls to unique IDs, keeping alc_responses\n",
    "control_unique = (\n",
    "    data_study2_control[['id', 'alc_responses']]\n",
    "    .drop_duplicates('id')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Compute percentage adherence\n",
    "control_unique['alc_responses_pct'] = (control_unique['alc_responses'] / 56) * 100\n",
    "\n",
    "# Summary statistics\n",
    "mean_pct = control_unique['alc_responses_pct'].mean()\n",
    "median_pct = control_unique['alc_responses_pct'].median()\n",
    "std_pct = control_unique['alc_responses_pct'].std()\n",
    "min_pct = control_unique['alc_responses_pct'].min()\n",
    "max_pct = control_unique['alc_responses_pct'].max()\n",
    "\n",
    "print(\"=== Study 2 Control Adherence (% of prompts completed) ===\")\n",
    "print(f\"Mean:   {mean_pct:.2f}%\")\n",
    "print(f\"Median: {median_pct:.2f}%\")\n",
    "print(f\"SD:     {std_pct:.2f}\")\n",
    "print(f\"Min:    {min_pct:.2f}%\")\n",
    "print(f\"Max:    {max_pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there demographic differences between responders and non-responders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split merged dataframe by responsiveness\n",
    "for resp_label, df_sub in merged.groupby('responsive'):\n",
    "    group_name = \"Responsive\" if resp_label == 1 else \"Non-responsive\"\n",
    "    print(f\"\\n=== {group_name} participants ===\")\n",
    "\n",
    "    # Age stats\n",
    "    age_mean = df_sub['age'].mean()\n",
    "    age_sd = df_sub['age'].std()\n",
    "    age_min = df_sub['age'].min()\n",
    "    age_max = df_sub['age'].max()\n",
    "    print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "\n",
    "    # Gender distribution\n",
    "    gender_counts = df_sub['gender_numeric'].value_counts(dropna=False).sort_index()\n",
    "    gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "    print(\"Gender distribution:\")\n",
    "    for gender, count in gender_counts.items():\n",
    "        label = \"Missing\" if pd.isna(gender) else f\"Gender {gender}\"\n",
    "        percent = gender_percent[gender]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "    # Responsive distribution (within this subgroup, should be homogeneous)\n",
    "    responsive_counts = df_sub['responsive'].value_counts(dropna=False).sort_index()\n",
    "    responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "    print(\"Responsive distribution:\")\n",
    "    for resp, count in responsive_counts.items():\n",
    "        label = \"Missing\" if pd.isna(resp) else f\"Responsive = {resp}\"\n",
    "        percent = responsive_percent[resp]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "    # Race distribution\n",
    "    race_counts = df_sub['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "    race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "    print(\"Race distribution:\")\n",
    "    for race, count in race_counts.items():\n",
    "        label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "        percent = race_percent[race]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "    # College year distribution\n",
    "    college_counts = df_sub['college_year'].value_counts(dropna=False).sort_index()\n",
    "    college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "    print(\"College year distribution:\")\n",
    "    for year, count in college_counts.items():\n",
    "        label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "        percent = college_percent[year]\n",
    "        print(f\"  {label}: {count} ({percent}%)\")\n",
    "\n",
    "    # Income stats\n",
    "    income_mean = df_sub['income_numeric'].mean()\n",
    "    income_sd = df_sub['income_numeric'].std()\n",
    "    income_median = df_sub['income_numeric'].median()\n",
    "    income_min = df_sub['income_numeric'].min()\n",
    "    income_max = df_sub['income_numeric'].max()\n",
    "    print(f\"Income: Mean = {round(income_mean, 2)}, SD = {round(income_sd, 2)}, Median = {income_median}, Range = [{income_min}, {income_max}]\")\n",
    "    print(f\"The household income for {group_name.lower()} participants was between {income_min} and {income_max}, with a median of {income_median}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge and filter ---\n",
    "study_2_response = data_study2.merge(b2_group_subjective_study2, on='id', how='left')\n",
    "\n",
    "# Compute intersection of valid IDs\n",
    "valid_ids = (\n",
    "    set(data_study2['id'])\n",
    "    & set(baseline_demo_study2['id'])\n",
    "    & set(b2_group_subjective_study2['id'])\n",
    ")\n",
    "print(\"Valid IDs:\", len(valid_ids))\n",
    "pd.Series(sorted(valid_ids), name=\"id\").to_csv(\"../data/study_2_ids.csv\", index=False)\n",
    "\n",
    "# Keep only participants present in all three datasets\n",
    "data_study2 = data_study2[data_study2['id'].isin(valid_ids)].copy()\n",
    "study_2_response = study_2_response[study_2_response['id'].isin(valid_ids)].copy()\n",
    "\n",
    "# Ensure drinks_number is numeric\n",
    "data_study2['drinks_number'] = pd.to_numeric(data_study2['drinks_number'], errors='coerce')\n",
    "\n",
    "# --- Aggregate per participant and week type ---\n",
    "weekly_stats_2 = (\n",
    "    data_study2\n",
    "    .groupby(['id', 'active_week'])\n",
    "    .agg(\n",
    "        total_drinks=('drinks_number', 'sum'),\n",
    "        occasions_number=('drinks_number', lambda x: (x > 0).sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary_2 = (\n",
    "    weekly_stats_2\n",
    "    .pivot(index='id', columns='active_week', values=['total_drinks', 'occasions_number'])\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "summary_2.columns = [f\"{col[0]}_{col[1]}\" for col in summary_2.columns]\n",
    "summary_2 = summary_2.reset_index()\n",
    "\n",
    "# --- Compute per-person averages ---\n",
    "summary_2['avg_drinks_active'] = summary_2['total_drinks_active'] / summary_2['occasions_number_active']\n",
    "summary_2['avg_drinks_control'] = summary_2['total_drinks_control'] / summary_2['occasions_number_control']\n",
    "summary_2['avg_occasions_active'] = summary_2['occasions_number_active'] / 2\n",
    "summary_2['avg_occasions_control'] = summary_2['occasions_number_control'] / 2\n",
    "\n",
    "# --- Merge with responsiveness info ---\n",
    "merged_final_2 = summary_2.merge(\n",
    "    responsive_study2[['id', 'responsive']],\n",
    "    on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Unique IDs in merged_final_2:\", merged_final_2['id'].nunique())\n",
    "\n",
    "\n",
    "merged_final_2.dropna(inplace=True)\n",
    "merged_final_2 = merged_final_2[merged_final_2['id'].isin(b2_group_subjective_study2['id'])]\n",
    "\n",
    "# --- Summary statistics ---\n",
    "stats_2 = (\n",
    "    merged_final_2\n",
    "    .groupby('responsive')[[\n",
    "        'avg_drinks_control', 'avg_drinks_active',\n",
    "        'avg_occasions_control', 'avg_occasions_active'\n",
    "    ]]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Flatten for readability\n",
    "stats_2.columns = [f\"{col[0]}_{col[1]}\" for col in stats_2.columns]\n",
    "stats_2 = stats_2.reset_index()\n",
    "print(stats_2)\n",
    "\n",
    "# --- Prepare plotting ---\n",
    "plot_df_2 = merged_final_2.melt(\n",
    "    id_vars='responsive',\n",
    "    value_vars=[\n",
    "        'avg_drinks_control', 'avg_drinks_active',\n",
    "        'avg_occasions_control', 'avg_occasions_active'\n",
    "    ],\n",
    "    var_name='measure',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "plot_df_2['type'] = plot_df_2['measure'].apply(lambda x: 'Drinks' if 'drinks' in x else 'Occasions')\n",
    "plot_df_2['condition'] = plot_df_2['measure'].apply(lambda x: 'Active' if 'active' in x else 'Control')\n",
    "\n",
    "palette = {0: 'lightgray', 1: 'darkgreen'}\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=False)\n",
    "\n",
    "# --- Drinks plot ---\n",
    "sns.boxplot(\n",
    "    data=plot_df_2[plot_df_2['type'] == 'Drinks'],\n",
    "    x='condition', y='value', hue='responsive',\n",
    "    ax=axes[0], palette=palette\n",
    ")\n",
    "axes[0].set_title('Average Drinks per Occasion')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Drinks per Occasion')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "axes[0].legend(handles, ['Non-responsive (0)', 'Responsive (1)'],\n",
    "               title='Responsive', facecolor='white', frameon=True)\n",
    "\n",
    "# --- Occasions plot ---\n",
    "sns.boxplot(\n",
    "    data=plot_df_2[plot_df_2['type'] == 'Occasions'],\n",
    "    x='condition', y='value', hue='responsive',\n",
    "    ax=axes[1], palette=palette\n",
    ")\n",
    "axes[1].set_title('Average Drinking Occasions per Week')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('Number of Weekly Occasions')\n",
    "axes[1].legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Welch’s t-tests ---\n",
    "tests_2 = [\n",
    "    ('avg_drinks_control', 'Control – Drinks per Occasion'),\n",
    "    ('avg_drinks_active', 'Active – Drinks per Occasion'),\n",
    "    ('avg_occasions_control', 'Control – Drinking Occasions per Week'),\n",
    "    ('avg_occasions_active', 'Active – Drinking Occasions per Week')\n",
    "]\n",
    "\n",
    "results_2 = []\n",
    "\n",
    "for var, label in tests_2:\n",
    "    res_vals = merged_final_2.loc[merged_final_2['responsive'] == 1, var].dropna()\n",
    "    nonres_vals = merged_final_2.loc[merged_final_2['responsive'] == 0, var].dropna()\n",
    "    t_stat, p_val = ttest_ind(res_vals, nonres_vals, equal_var=False)\n",
    "    results_2.append({\n",
    "        \"Measure\": label,\n",
    "        \"Responsive Mean\": res_vals.mean(),\n",
    "        \"Non-responsive Mean\": nonres_vals.mean(),\n",
    "        \"t\": t_stat,\n",
    "        \"p\": p_val\n",
    "    })\n",
    "\n",
    "test_df_2 = pd.DataFrame(results_2).round(3)\n",
    "test_df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final_2 = summary_2.merge(\n",
    "    responsive_study2[['id', 'responsive']],\n",
    "    on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# --- Count responders vs non-responders ---\n",
    "resp_counts = merged_final_2['responsive'].value_counts().rename_axis('responsive').reset_index(name='count')\n",
    "print(\"\\nNumber of responsive vs non-responsive participants:\")\n",
    "print(resp_counts)\n",
    "print(f\"Total: {resp_counts['count'].sum()} participants\")\n",
    "\n",
    "print(\"Unique IDs in merged_final_2:\", merged_final_2['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Mindful vs Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindful_ids_study2 = study_2_response.loc[study_2_response[\"condition\"] == \"mindful\", \"id\"].unique()\n",
    "perspective_ids_study2 = study_2_response.loc[study_2_response[\"condition\"] == \"perspective\", \"id\"].unique()\n",
    "\n",
    "print(\"Mindful IDs:\", len(mindful_ids_study2))\n",
    "print(\"Perspective IDs:\", len(perspective_ids_study2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_subgroup(df, label):\n",
    "    print(f\"\\n=== {label} Group ===\")\n",
    "    age_mean = df['age'].mean()\n",
    "    age_sd = df['age'].std()\n",
    "    age_min = df['age'].min()\n",
    "    age_max = df['age'].max()\n",
    "    print(f\"Age: M = {round(age_mean, 2)}, SD = {round(age_sd, 2)}, Min = {age_min}, Max = {age_max}\")\n",
    "\n",
    "    gender_counts = df['gender_numeric'].value_counts(dropna=False).sort_index()\n",
    "    gender_percent = round(100 * gender_counts / gender_counts.sum(), 1)\n",
    "    print(\"Gender distribution:\")\n",
    "    for gender, count in gender_counts.items():\n",
    "        label = \"Missing\" if pd.isna(gender) else f\"Gender {gender}\"\n",
    "        print(f\"  {label}: {count} ({gender_percent[gender]}%)\")\n",
    "\n",
    "    responsive_counts = df['responsive'].value_counts(dropna=False).sort_index()\n",
    "    responsive_percent = round(100 * responsive_counts / responsive_counts.sum(), 1)\n",
    "    print(\"Responsive distribution:\")\n",
    "    for resp, count in responsive_counts.items():\n",
    "        label = \"Missing\" if pd.isna(resp) else f\"Responsive = {resp}\"\n",
    "        print(f\"  {label}: {count} ({responsive_percent[resp]}%)\")\n",
    "\n",
    "    race_counts = df['race_numeric'].value_counts(dropna=False).sort_index()\n",
    "    race_percent = round(100 * race_counts / race_counts.sum(), 1)\n",
    "    print(\"Race distribution:\")\n",
    "    for race, count in race_counts.items():\n",
    "        label = \"Missing\" if pd.isna(race) else f\"Race {race}\"\n",
    "        print(f\"  {label}: {count} ({race_percent[race]}%)\")\n",
    "\n",
    "    college_counts = df['college_year'].value_counts(dropna=False).sort_index()\n",
    "    college_percent = round(100 * college_counts / college_counts.sum(), 1)\n",
    "    print(\"College year distribution:\")\n",
    "    for year, count in college_counts.items():\n",
    "        label = \"Missing\" if pd.isna(year) else f\"Year {year}\"\n",
    "        print(f\"  {label}: {count} ({college_percent[year]}%)\")\n",
    "\n",
    "    income_mean = round(df['income_numeric'].mean(), 2)\n",
    "    income_sd = round(df['income_numeric'].std(), 2)\n",
    "    income_median = df['income_numeric'].median()\n",
    "    income_min = df['income_numeric'].min()\n",
    "    income_max = df['income_numeric'].max()\n",
    "    print(f\"Income: Median = {income_median}, Mean = {income_mean}, SD = {income_sd}\")\n",
    "    print(f\"The household income ranged between {income_min} and {income_max}.\\n\")\n",
    "\n",
    "# --- Subset by intervention type ---\n",
    "mindful_df = merged[merged['id'].isin(mindful_ids_study2)]\n",
    "perspective_df = merged[merged['id'].isin(perspective_ids_study2)]\n",
    "\n",
    "# --- Run for both groups ---\n",
    "describe_subgroup(mindful_df, \"Mindfulness\")\n",
    "describe_subgroup(perspective_df, \"Perspective-taking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adherence by intervention type\n",
    "cond_map = study_2_response[['id', 'condition']].drop_duplicates('id')\n",
    "\n",
    "# --- Merge condition with adherence ---\n",
    "merged_df = df_adherence_2.merge(cond_map, on='id', how='inner')\n",
    "\n",
    "# --- Check ---\n",
    "print(merged_df['condition'].value_counts(dropna=False))\n",
    "\n",
    "# --- Compute adherence stats per condition ---\n",
    "adherence_stats = (\n",
    "    merged_df\n",
    "    .groupby('condition')['alc_responses']\n",
    "    .agg(['mean', 'std', 'min', 'max'])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(adherence_stats/56)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline drinking levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load valid IDs\n",
    "valid_ids = pd.read_csv(\"../data/study_2_ids.csv\")['id']\n",
    "\n",
    "# Filter response dataframe\n",
    "response = responsive_study2[\n",
    "    responsive_study2['id'].isin(valid_ids)\n",
    "]\n",
    "\n",
    "# Filter baseline alcohol self-report dataframe\n",
    "baseline_alc_self = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(valid_ids)\n",
    "]\n",
    "\n",
    "# Merge responder status\n",
    "baseline_alc_self = baseline_alc_self.merge(\n",
    "    response[['id', 'responsive']],\n",
    "    left_on='pID',\n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Count how many remain after merge\n",
    "print(\"Participants after merge:\", baseline_alc_self['pID'].nunique())\n",
    "print(\"Rows after merge:\", len(baseline_alc_self))\n",
    "\n",
    "# Define numeric columns\n",
    "alc_self_cols = ['freq_self', 'amount_self']\n",
    "\n",
    "# Convert to numeric\n",
    "for c in alc_self_cols:\n",
    "    baseline_alc_self[c] = pd.to_numeric(baseline_alc_self[c], errors='coerce')\n",
    "\n",
    "# Remove invalid (<0) values\n",
    "baseline_alc_self = baseline_alc_self[\n",
    "    (baseline_alc_self['freq_self'] >= 0) &\n",
    "    (baseline_alc_self['amount_self'] >= 0)\n",
    "]\n",
    "\n",
    "print(\"Participants after removing invalid values:\", baseline_alc_self['pID'].nunique())\n",
    "print(\"Rows after removing invalid:\", len(baseline_alc_self))\n",
    "\n",
    "# Split by responder status\n",
    "baseline_responders = baseline_alc_self[baseline_alc_self['responsive'] == 1]\n",
    "baseline_non_responders = baseline_alc_self[baseline_alc_self['responsive'] == 0]\n",
    "\n",
    "# Summary stats\n",
    "baseline_responder_stats = (\n",
    "    baseline_responders[alc_self_cols]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .T\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "baseline_non_responder_stats = (\n",
    "    baseline_non_responders[alc_self_cols]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .T\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"=== Baseline self-report: Responders ===\")\n",
    "print(baseline_responder_stats)\n",
    "print(\"\\n=== Baseline self-report: Non-Responders ===\")\n",
    "print(baseline_non_responder_stats)\n",
    "\n",
    "# Mann-Whitney U\n",
    "for col in alc_self_cols:\n",
    "    x = baseline_responders[col].dropna()\n",
    "    y = baseline_non_responders[col].dropna()\n",
    "    stat, p = mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "    print(f\"{col}: U = {stat:.2f}, p = {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique controls with IDs\n",
    "control_unique = (\n",
    "    data_study2_control[['id', 'alc_responses']]\n",
    "    .drop_duplicates('id')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "control_ids = control_unique[['id']].drop_duplicates('id')\n",
    "\n",
    "# Baseline rows for controls\n",
    "baseline_controls = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(control_ids['id'])\n",
    "].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "for c in ['freq_self', 'amount_self']:\n",
    "    baseline_controls[c] = pd.to_numeric(baseline_controls[c], errors='coerce')\n",
    "\n",
    "# Remove invalid (<0)\n",
    "baseline_controls = baseline_controls[\n",
    "    (baseline_controls['freq_self'] >= 0) &\n",
    "    (baseline_controls['amount_self'] >= 0)\n",
    "]\n",
    "\n",
    "print(\"Control participants:\", baseline_controls['pID'].nunique())\n",
    "print(\"Control rows:\", len(baseline_controls))\n",
    "\n",
    "baseline_intervention = baseline_alc_self[\n",
    "    baseline_alc_self['responsive'].isin([0, 1])\n",
    "].copy()\n",
    "\n",
    "control_stats = (\n",
    "    baseline_controls[['freq_self', 'amount_self']]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .T.round(2)\n",
    ")\n",
    "\n",
    "print(\"=== Baseline self-report: Controls ===\")\n",
    "print(control_stats)\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "for col in ['freq_self', 'amount_self']:\n",
    "    x = baseline_controls[col].dropna()\n",
    "    y = baseline_intervention[col].dropna()\n",
    "    stat, p = mannwhitneyu(x, y, alternative='two-sided')\n",
    "    print(f\"[MWU] {col}: U = {stat:.2f}, p = {p:.3f}\")\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "for col in ['freq_self', 'amount_self']:\n",
    "    x = baseline_controls[col].dropna()\n",
    "    y = baseline_intervention[col].dropna()\n",
    "    t, p = ttest_ind(x, y, equal_var=False)\n",
    "    print(f\"[Welch t-test] {col}: t = {t:.2f}, p = {p:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "\n",
    "# === 1. Load fresh data ===\n",
    "\n",
    "# Baseline alcohol self-report (all participants, all studies)\n",
    "baseline_alc_self_all = pd.read_csv(\n",
    "    \"/Users/fmagdalena/Documents/GitHub/SHINE-responsiveness-analysis/data/baseline/00_baseline_survey.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "# Study 2 responsiveness file (intervention arms)\n",
    "responsive_study2 = pd.read_csv(\"../data/intervention_time/responsiveness_study2.csv\")\n",
    "\n",
    "# Valid Study 2 intervention IDs (intersection you saved earlier)\n",
    "valid_ids = pd.read_csv(\"../data/study_2_ids.csv\")['id']\n",
    "\n",
    "# === 2. Build intervention baseline df (Study 2, responders + non-responders) ===\n",
    "\n",
    "baseline_alc_self = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(valid_ids)\n",
    "].copy()\n",
    "\n",
    "baseline_alc_self = baseline_alc_self.merge(\n",
    "    responsive_study2[['id', 'responsive']],\n",
    "    left_on='pID',\n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Intervention participants after merge:\", baseline_alc_self['pID'].nunique())\n",
    "print(\"Intervention rows after merge:\", len(baseline_alc_self))\n",
    "\n",
    "alc_self_cols = ['freq_self', 'amount_self']\n",
    "\n",
    "for c in alc_self_cols:\n",
    "    baseline_alc_self[c] = pd.to_numeric(baseline_alc_self[c], errors='coerce')\n",
    "\n",
    "baseline_alc_self = baseline_alc_self[\n",
    "    (baseline_alc_self['freq_self'] >= 0) &\n",
    "    (baseline_alc_self['amount_self'] >= 0)\n",
    "]\n",
    "\n",
    "print(\"Intervention participants after cleaning:\", baseline_alc_self['pID'].nunique())\n",
    "print(\"Intervention rows after cleaning:\", len(baseline_alc_self))\n",
    "\n",
    "baseline_intervention = baseline_alc_self[\n",
    "    baseline_alc_self['responsive'].isin([0, 1])\n",
    "].copy()\n",
    "\n",
    "# === 3. Build control baseline df (Study 2 controls) ===\n",
    "\n",
    "# data_study2_control is assumed to already be loaded elsewhere\n",
    "control_unique = (\n",
    "    data_study2_control[['id', 'alc_responses']]\n",
    "    .drop_duplicates('id')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(\"Unique control IDs in data_study2_control:\", control_unique['id'].nunique())\n",
    "\n",
    "baseline_controls = baseline_alc_self_all[\n",
    "    baseline_alc_self_all['pID'].isin(control_unique['id'])\n",
    "].copy()\n",
    "\n",
    "for c in alc_self_cols:\n",
    "    baseline_controls[c] = pd.to_numeric(baseline_controls[c], errors='coerce')\n",
    "\n",
    "baseline_controls = baseline_controls[\n",
    "    (baseline_controls['freq_self'] >= 0) &\n",
    "    (baseline_controls['amount_self'] >= 0)\n",
    "]\n",
    "\n",
    "print(\"Control participants with baseline alcohol data:\", baseline_controls['pID'].nunique())\n",
    "print(\"Control rows after cleaning:\", len(baseline_controls))\n",
    "\n",
    "# === 4. Summary stats for controls ===\n",
    "\n",
    "control_stats = (\n",
    "    baseline_controls[alc_self_cols]\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .T.round(2)\n",
    ")\n",
    "\n",
    "print(\"=== Baseline self-report: Controls ===\")\n",
    "print(control_stats)\n",
    "\n",
    "# === 5. MWU and Welch t-tests: controls vs intervention (pooled) ===\n",
    "\n",
    "for col in alc_self_cols:\n",
    "    x = baseline_controls[col].dropna()\n",
    "    y = baseline_intervention[col].dropna()\n",
    "\n",
    "    print(f\"\\n=== {col} ===\")\n",
    "    print(\"n_controls:\", len(x), \"n_intervention:\", len(y))\n",
    "\n",
    "    # Mann–Whitney U\n",
    "    stat_u, p_u = mannwhitneyu(x, y, alternative='two-sided')\n",
    "    print(f\"[MWU] U = {stat_u:.2f}, p = {p_u:.3f}\")\n",
    "\n",
    "    # Welch t-test\n",
    "    t, p_t = ttest_ind(x, y, equal_var=False)\n",
    "    print(f\"[Welch t-test] t = {t:.2f}, p = {p_t:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_group_ids = baseline_alc_self_all[['pID', 'group_type', 'group_gender', 'groupID']]\n",
    "ids_study_1 = pd.read_csv('../data/study_1_ids.csv')\n",
    "ids_study_2 = pd.read_csv('../data/study_2_ids.csv')\n",
    "\n",
    "# baseline_group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent ID column names\n",
    "ids_study_1 = ids_study_1.rename(columns={\"id\": \"pID\"})\n",
    "ids_study_2 = ids_study_2.rename(columns={\"id\": \"pID\"})\n",
    "\n",
    "# Filter baseline to each study\n",
    "study1 = baseline_group_ids[baseline_group_ids[\"pID\"].isin(ids_study_1[\"pID\"])]\n",
    "study2 = baseline_group_ids[baseline_group_ids[\"pID\"].isin(ids_study_2[\"pID\"])]\n",
    "\n",
    "def summarize(study_df, label):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "\n",
    "    # Unique groups\n",
    "    n_groups = study_df[\"groupID\"].nunique()\n",
    "    print(\"Total unique groups:\", n_groups)\n",
    "\n",
    "    # Groups by type\n",
    "    groups_by_type = (\n",
    "        study_df.drop_duplicates(\"groupID\")\n",
    "        .groupby(\"group_type\")[\"groupID\"]\n",
    "        .nunique()\n",
    "    )\n",
    "    print(\"\\nGroups by type:\")\n",
    "    print(groups_by_type)\n",
    "\n",
    "    # Groups by gender\n",
    "    groups_by_gender = (\n",
    "        study_df.drop_duplicates(\"groupID\")\n",
    "        .groupby(\"group_gender\")[\"groupID\"]\n",
    "        .nunique()\n",
    "    )\n",
    "    print(\"\\nGroups by gender:\")\n",
    "    print(groups_by_gender)\n",
    "\n",
    "summarize(study1, \"Study 1\")\n",
    "summarize(study2, \"Study 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "def run_rf_train_test(dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5, model_choice_metric='f1', \n",
    "                      res_dir=f\"./results/\", model_type='xgb', test_set=0.3, permutation=False, random_state=321):\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{SEED}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    keys = list(dataframes.keys())\n",
    "\n",
    "    # combine data categories\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {} \n",
    "    best_shap_vals = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "        \n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "    \n",
    "        all_shap_values = []\n",
    "        all_test_data = []\n",
    "        best_overall_score = -np.inf \n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "        best_shap_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps): # i repetitions of train/test\n",
    "\n",
    "            # Prepare train/test split for this i (random & stratified)\n",
    "            X_data, Y_data, X_test, Y_test, x_test_index = prepare_features_and_targets(merged_df.copy(), test_set=test_set, target_var=TARGET_VAR)\n",
    "\n",
    "            # Shuffle labels for permutation tests\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            best_model, best_scores, best_params = random_forest_kfold_grid_search(X_data, Y_data, \n",
    "                                                                                    param_grid, k=k, \n",
    "                                                                                    CV_reps=CV_reps, \n",
    "                                                                                    eval_metric=eval_metrics,\n",
    "                                                                                    model_choice_metric=model_choice_metric,\n",
    "                                                                                    res_dir=res_dir,\n",
    "                                                                                    model_type=model_type,\n",
    "                                                                                    combo=combo, \n",
    "                                                                                    random_state=random_state)\n",
    "            # Collect metrics\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain the best model on the full training dataset and evaluate on the test set\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            pred_df = pd.DataFrame({\n",
    "                \"id\": x_test_index,\n",
    "                \"predicted_probability\": proba_predictions\n",
    "            })\n",
    "\n",
    "            # Save to CSV\n",
    "            out_path = \"./predicted_probabilities.csv\"\n",
    "            pred_df.to_csv(out_path, mode=\"a\", header=not os.path.exists(out_path), index=False)\n",
    "\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test) \n",
    "            shap_values = shap_values[:, :, 1]\n",
    "\n",
    "            # Append SHAP values and test data for later aggregation\n",
    "            all_shap_values.append(shap_values)\n",
    "            all_test_data.append(pd.DataFrame(X_test))\n",
    "            print(f\"Xtest columns: {X_test.columns}\")\n",
    "\n",
    "            if best_scores[model_choice_metric] > best_overall_score:\n",
    "                best_overall_score = best_scores[model_choice_metric]\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "                best_shap_for_combo = shap_values  # Store SHAP values if needed\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores[model_choice_metric], deepcopy(best_model)))\n",
    "\n",
    "            # Calculate and append metrics for the test set\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep track of the best model based on the model_choice_metric\n",
    "        if combo not in best_models or best_scores[model_choice_metric] > combo_validation_scores[combo][model_choice_metric]['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            \n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "\n",
    "            best_shap_vals[combo] = best_shap_for_combo\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            # Save top 10 models for group_sub combo\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "\n",
    "                # Sort and save top 10 by score\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:100]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top100_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "\n",
    "                # Store back in locals so it's not overwritten each time\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        top2_features = plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo)\n",
    "\n",
    "        if not model_type=='dt':\n",
    "            plot_pdp_across_runs(\n",
    "                best_model=best_model_for_combo,\n",
    "                res_dir=res_dir,\n",
    "                all_test_data=all_test_data,\n",
    "                interaction_pair=tuple(top2_features),\n",
    "                ids = []\n",
    "            )\n",
    "\n",
    "        # Calculate mean and 95% CI for validation scores\n",
    "        z = norm.ppf(0.975)  # 95% confidence level\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        # Calculate mean and 95% CI for test scores\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        # For validation scores\n",
    "        df_val = flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        # For test scores\n",
    "        df_test = flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "        \n",
    "    return res_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_train_test(\n",
    "    dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5,\n",
    "    model_choice_metric='roc_auc', res_dir=f\"./results_svm/\",\n",
    "    model_type='svm', test_set=0.3, permutation=False, random_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Mirrors your LR runner, but with SVM. Saves top 100 models for ('group_sub',).\n",
    "    \"\"\"\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{random_state}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "    keys = list(dataframes.keys())\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "\n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "\n",
    "        best_overall_score = -np.inf\n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps):\n",
    "            # Train/test split\n",
    "            X_data, Y_data, X_test, Y_test = prepare_features_and_targets(merged_df.copy(), test_set=test_set, target_var=TARGET_VAR)\n",
    "\n",
    "            # Optional permutation\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            # Inner CV selection\n",
    "            best_model, best_scores, best_params = svm_kfold_grid_search(\n",
    "                X_data, Y_data, param_grid, k=k, CV_reps=CV_reps,\n",
    "                eval_metric=eval_metrics, model_choice_metric=model_choice_metric, combo=combo\n",
    "            )\n",
    "\n",
    "            # Record inner-CV selection metric(s)\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain on full training data and evaluate on test\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            if best_scores.get(model_choice_metric, -np.inf) > best_overall_score:\n",
    "                best_overall_score = best_scores.get(model_choice_metric, -np.inf)\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores.get(model_choice_metric, -np.inf), deepcopy(best_model)))\n",
    "\n",
    "            # Accumulate test metrics\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep best model; save\n",
    "        if combo not in best_models or best_overall_score > combo_validation_scores.get(combo, {}).get(model_choice_metric, {'mean': -np.inf})['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:100]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top100_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        # Summary stats (mean, normal-approx 95% CI)\n",
    "        z = norm.ppf(0.975)\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            if len(scores) == 0 or np.all(pd.isna(scores)):\n",
    "                mean_score, ci_lower, ci_upper = np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                mean_score = np.nanmean(scores)\n",
    "                std_error = np.nanstd(scores, ddof=1) / np.sqrt(np.sum(~pd.isna(scores)))\n",
    "                ci_lower = mean_score - z * std_error\n",
    "                ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {'mean': mean_score, '95%_CI': (ci_lower, ci_upper)}\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            if len(scores) == 0 or np.all(pd.isna(scores)):\n",
    "                mean_score, ci_lower, ci_upper = np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                mean_score = np.nanmean(scores)\n",
    "                std_error = np.nanstd(scores, ddof=1) / np.sqrt(np.sum(~pd.isna(scores)))\n",
    "                ci_lower = mean_score - z * std_error\n",
    "                ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {'mean': mean_score, '95%_CI': (ci_lower, ci_upper)}\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "\n",
    "    return res_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "def run_lr_train_test(dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5, model_choice_metric='f1', \n",
    "                      res_dir=f\"./results/\", model_type='xgb', test_set=0.3, permutation=False):\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{SEED}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    keys = list(dataframes.keys())\n",
    "\n",
    "    # combine data categories\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {} \n",
    "    best_shap_vals = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "        \n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "    \n",
    "        all_shap_values = []\n",
    "        all_test_data = []\n",
    "        best_overall_score = -np.inf \n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "        best_shap_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps): # i repetitions of train/test\n",
    "\n",
    "            # Prepare train/test split for this i (random & stratified)\n",
    "            X_data, Y_data, X_test, Y_test = prepare_features_and_targets(merged_df.copy(), test_set=test_set, target_var=TARGET_VAR)\n",
    "\n",
    "            # Shuffle labels for permutation tests\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            best_model, best_scores, best_params = logistic_regression_kfold_grid_search(X_data, Y_data, \n",
    "                                                                                    param_grid, k=k, \n",
    "                                                                                    CV_reps=CV_reps, \n",
    "                                                                                    eval_metric=eval_metrics,\n",
    "                                                                                    model_choice_metric=model_choice_metric,\n",
    "                                                                                    res_dir=res_dir,\n",
    "                                                                                    combo=combo)\n",
    "            # Collect metrics\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain the best model on the full training dataset and evaluate on the test set\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "            all_test_data.append(pd.DataFrame(X_test))\n",
    "\n",
    "            if best_scores[model_choice_metric] > best_overall_score:\n",
    "                best_overall_score = best_scores[model_choice_metric]\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "                # best_shap_for_combo = shap_values  # Store SHAP values if needed\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores[model_choice_metric], deepcopy(best_model)))\n",
    "\n",
    "            # Calculate and append metrics for the test set\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep track of the best model based on the model_choice_metric\n",
    "        if combo not in best_models or best_scores[model_choice_metric] > combo_validation_scores[combo][model_choice_metric]['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            \n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "\n",
    "            # best_shap_vals[combo] = best_shap_for_combo\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            # Save top 10 models for group_sub combo\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "\n",
    "                # Sort and save top 10 by score\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:100]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top100_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "\n",
    "                # Store back in locals so it's not overwritten each time\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        # Calculate mean and 95% CI for validation scores\n",
    "        z = norm.ppf(0.975)  # 95% confidence level\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        # Calculate mean and 95% CI for test scores\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        # For validation scores\n",
    "        df_val = flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        # For test scores\n",
    "        df_test = flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "        \n",
    "    return res_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_alcohol_self_response.dropna(inplace=True)\n",
    "b6_psychometric_response.dropna(inplace=True)\n",
    "b2_group_subjective_response.dropna(inplace=True)\n",
    "b4_brain_response.dropna(inplace=True)\n",
    "b3_group_sociometric_response.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'demo': b5_demographic_response,\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'psych': b6_psychometric_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response\n",
    "    # 'group_selfreport': b7_objective_group_drinking_response\n",
    "}\n",
    "\n",
    "eval_metrics = ['auc', 'f1', 'accuracy', 'specificity', 'sensitivity', 'PPV', 'NPV', 'MCC', 'balancedAcc', 'pr_auc', 'tn', 'fn', 'tp', 'fp']\n",
    "p_value_metrics = ['auc', 'f1', 'specificity', 'sensitivity', 'PPV', 'NPV', 'balancedAcc', 'pr_auc',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [2, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "# res_dir = run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=20, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results_perspective_only/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Permutation Test\n",
    "# perm_dir = run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=f\"{res_dir}/permutation_test\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External test set\n",
    "res_dir = \"../results_mindful_only/1762426109_321_rf_outer20_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "loaded_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "scores, best_params = test_oos(b2_group_subjective_test, res_dir, loaded_model, None, plot=True, target_var=TARGET_VAR)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"penalty\": [\"elasticnet\"],\n",
    "#     \"solver\": [\"saga\"],\n",
    "#     \"C\": [1e-2, 1e-1, 1.0],\n",
    "#     \"l1_ratio\": [0.0, 0.5, 1.0],\n",
    "#     \"max_iter\": [500],\n",
    "#     \"class_weight\": [\"balanced\"],\n",
    "# }\n",
    "\n",
    "# res_dir = run_lr_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results_logreg/\",\n",
    "#     model_type='lr_elasticnet',\n",
    "#     test_set=0.3,\n",
    "#     permutation=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_dir = run_lr_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=f\"{res_dir}/permutation_test\",\n",
    "#     model_type='lr_elasticnet',\n",
    "#     test_set=0.3,\n",
    "#     permutation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"../results_logreg/1760898281_321_lr_elasticnet_outer100_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "lr_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "\n",
    "scores, best_params, feature_importance = test_oos_logreg(\n",
    "    b2_group_subjective_test,\n",
    "    res_dir,\n",
    "    lr_model,\n",
    "    target_var=TARGET_VAR,\n",
    "    plot=False,\n",
    "    n_perm_repeats=1000\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"../results_logreg/1760879992_321_lr_l1l2_outer100_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "loaded_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "scores, best_params, importance = test_oos_logreg(b2_group_subjective_test, res_dir, loaded_model, None, plot=False, target_var=TARGET_VAR, n_perm_repeats=100)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"kernel\": [\"rbf\"],\n",
    "#     \"C\": [1e-3, 1e-2, 1e-1, 1, 10],\n",
    "#     \"gamma\": [\"scale\", 1e-3, 1e-2, 1e-1],\n",
    "# }\n",
    "\n",
    "# res_dir = run_svm_train_test(\n",
    "#     dataframes=dataframes,             # dict of dfs keyed by feature group names\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',     # sklearn scorer key for inner CV selection\n",
    "#     res_dir=\"../results_svm_rbf/\",\n",
    "#     model_type='svm_rbf',\n",
    "#     test_set=0.30,\n",
    "#     permutation=False,\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Permutation test\n",
    "# perm_dir = run_svm_train_test(\n",
    "#     dataframes=dataframes,             # dict of dfs keyed by feature group names\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',     # sklearn scorer key for inner CV selection\n",
    "#     res_dir=f\"{res_dir}/permutation_test\",\n",
    "#     model_type='svm_rbf',\n",
    "#     test_set=0.30,\n",
    "#     permutation=True,\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"../results_svm_rbf//1760883787_321_svm_rbf_outer100_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "loaded_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "scores, best_params, importance = test_oos_svm(b2_group_subjective_test, res_dir, loaded_model, None, plot=False, target_var=TARGET_VAR, n_perm_repeats=1000)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"kernel\": [\"linear\"],\n",
    "#     \"C\": [1e-2, 1e-1, 1, 10],\n",
    "# }\n",
    "\n",
    "# res_dir = run_svm_train_test(\n",
    "#     dataframes=dataframes,         # dict of dfs keyed by feature group names\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',     # sklearn scorer key for inner CV selection\n",
    "#     res_dir=\"../results_svm_linear/\",\n",
    "#     model_type='svm_linear',\n",
    "#     test_set=0.30,\n",
    "#     permutation=False,\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Permutation test\n",
    "\n",
    "# perm_dir = run_svm_train_test(\n",
    "#     dataframes=dataframes,         # dict of dfs keyed by feature group names\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',     # sklearn scorer key for inner CV selection\n",
    "#     res_dir=f\"{res_dir}/permutation_test\",\n",
    "#     model_type='svm_linear',\n",
    "#     test_set=0.30,\n",
    "#     permutation=True,\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"../results_svm_linear//1760889998_321_svm_linear_outer100_cvrep5_k3_auc_testsize0.3_permFalse\"\n",
    "loaded_model = joblib.load(f'{res_dir}/model_group_sub.joblib')\n",
    "\n",
    "res_dir = os.path.join(res_dir, 'oos_test') \n",
    "scores, best_params, importance = test_oos_svm(b2_group_subjective_test, res_dir, loaded_model, None, plot=True, target_var=TARGET_VAR, n_perm_repeats=1000)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import joblib\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define paths\n",
    "# model_dir = \"../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse_og/top10_group_sub_models\"\n",
    "# model_files = sorted([f for f in os.listdir(model_dir) if f.endswith(\".joblib\")])\n",
    "\n",
    "# # Storage for all permutation scores\n",
    "# perm_model_scores = []\n",
    "\n",
    "# # Evaluate each model on the real test set\n",
    "# for model_file in tqdm(model_files):\n",
    "#     model_path = os.path.join(model_dir, model_file)\n",
    "#     loaded_model = joblib.load(model_path)\n",
    "    \n",
    "#     # Run model on the original test set\n",
    "#     scores, _ = test_oos(b2_group_subjective_test, model_dir, loaded_model, None, plot=False, target_var=TARGET_VAR)\n",
    "    \n",
    "#     # Track scores\n",
    "#     scores['model_file'] = model_file\n",
    "#     perm_model_scores.append(scores)\n",
    "\n",
    "# # Create DataFrame of permutation scores\n",
    "# perm_model_df = pd.DataFrame(perm_model_scores)\n",
    "\n",
    "# # Compare each metric in summary_df to the permutation distribution\n",
    "# comparison_results = {}\n",
    "# for metric in summary_df.index:\n",
    "#     if metric not in perm_model_df.columns:\n",
    "#         continue\n",
    "#     threshold = summary_df.loc[metric, 'Mean']\n",
    "#     count = (perm_model_df[metric] >= threshold).sum()\n",
    "#     comparison_results[metric] = {\n",
    "#         'NumPermutations >= True': count,\n",
    "#         'Proportion': count / len(perm_model_df)\n",
    "#     }\n",
    "\n",
    "# # Format as DataFrame\n",
    "# perm_test_summary = pd.DataFrame(comparison_results).T\n",
    "# print(perm_test_summary.sort_values(\"Proportion\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# n_perm = len(perm_model_df)\n",
    "\n",
    "# records, pvals = [], []\n",
    "# for metric in summary_df.index:\n",
    "#     if metric not in perm_model_df.columns:\n",
    "#         continue\n",
    "#     thr = summary_df.loc[metric, 'Mean']\n",
    "#     count = int((perm_model_df[metric] >= thr).sum())\n",
    "#     prop = count / n_perm\n",
    "#     p = (count + 1) / (n_perm + 1)  # permutation p with +1 correction\n",
    "\n",
    "#     records.append({\n",
    "#         'Metric': metric,\n",
    "#         'Threshold': thr,\n",
    "#         'NumPermutations >= Threshold': count,\n",
    "#         'Proportion': prop,\n",
    "#         'p_value': p\n",
    "#     })\n",
    "#     pvals.append(p)\n",
    "\n",
    "# # FDR correction across metrics\n",
    "# _, qvals, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# # Attach corrected p values\n",
    "# for rec, q in zip(records, qvals):\n",
    "#     rec['p_value_fdr_bh'] = q\n",
    "\n",
    "# perm_test_summary = pd.DataFrame(records).set_index('Metric').sort_values('p_value_fdr_bh')\n",
    "# print(perm_test_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CV with different values for k and different train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sensitivity \n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.4,\n",
    "#     permutation=False\n",
    "# )\n",
    "\n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=5,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"../results/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_dir = run_svm_train_test(\n",
    "#     dataframes=dataframes,             # dict of dfs keyed by feature group names\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='sensitivity',     # sklearn scorer key for inner CV selection\n",
    "#     res_dir=\"../results_svm/\",\n",
    "#     model_type='svm',\n",
    "#     test_set=0.30,\n",
    "#     permutation=False,\n",
    "#     random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_dir = run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100, # reduce for faster run --> this affects the results\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='sensitivity',\n",
    "#     res_dir=\"../results/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     permutation=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
