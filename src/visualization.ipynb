{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Visualization\n",
    "\n",
    "This notebook produces the main visualization (Figure 3) of validation results, and computes p-values for results compared to the permutation tests. In order to run this, specify the folder name of the run you want to test, e.g. `1747831228_rf_outer3_cvrep5_k3_auc_testsize0.3_permFalse` as `folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandlerText(HandlerBase):\n",
    "    def create_artists(self, legend, orig_handle, x0, y0, width, height, fontsize, trans):\n",
    "        t = Text(x=x0, y=y0, text=orig_handle.get_text(), color='red', fontsize=fontsize)\n",
    "        return [t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = '1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse_og'\n",
    "# RES_DIR = os.path.join('../results', folder)\n",
    "\n",
    "folder = '1762426109_321_rf_outer20_cvrep5_k3_auc_testsize0.3_permFalse'\n",
    "RES_DIR = os.path.join('../results_mindful_only/', folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(res_dir):\n",
    "    \"\"\"\n",
    "    Plots model performance comparing permutation vs non-permutation runs.\n",
    "    \n",
    "    Args:\n",
    "        res_dir (str): Results directory for the run to visualize\n",
    "    \"\"\"\n",
    "\n",
    "    perm_folder = os.path.join(res_dir, 'permutation_test', )\n",
    "\n",
    "    perm_folder = os.path.join(res_dir, 'permutation_test')\n",
    "    subfolders = [f for f in os.listdir(perm_folder) if os.path.isdir(os.path.join(perm_folder, f))]\n",
    "    assert len(subfolders) == 1, \"Expected exactly one subfolder\"\n",
    "    perm_folder = os.path.join(perm_folder, subfolders[0])\n",
    "\n",
    "\n",
    "    test_scores_path = os.path.join(res_dir, \"test_scores.csv\")\n",
    "    all_test_scores_nonperm_path = os.path.join(res_dir, \"all_test_scores.csv\")\n",
    "    all_test_scores_perm_path = os.path.join(perm_folder, \"all_test_scores.csv\")\n",
    "\n",
    "    # Existing plotting logic goes here, using the three files above\n",
    "\n",
    "    df = pd.read_csv(test_scores_path)\n",
    "    perm_df = pd.read_csv(all_test_scores_perm_path)\n",
    "    all_test_df = pd.read_csv(all_test_scores_nonperm_path)\n",
    "\n",
    "    perm_df[\"group\"] = perm_df[\"group\"].apply(ast.literal_eval)\n",
    "    all_test_df[\"group\"] = all_test_df[\"group\"].apply(ast.literal_eval)\n",
    "\n",
    "    factor_map = {\n",
    "        \"group_sub\": \"PEER\", \"alc_self\": \"ALC\", \"psych\": \"PSY\",\n",
    "        \"group_socio\": \"SOC\", \"brain\": \"NR\", \"demo\": \"D\"\n",
    "    }\n",
    "\n",
    "    df[\"Factor_Label\"] = df.apply(\n",
    "        lambda row: \"-\".join(sorted([factor_map.get(f, f) for f in [row[\"Factor_1\"], row[\"Factor_2\"]] if pd.notna(f)])),\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"Factor_Label values in df:\", df[\"Factor_Label\"].unique())\n",
    "\n",
    "    factor_order = [\n",
    "        \"PSY-SOC\", \"NR-SOC\", \"NR-PSY\",\n",
    "        \"ALC-NR\", \"ALC-SOC\", \"ALC-PSY\", \"ALC-D\", \"D-NR\", \"D-SOC\", \n",
    "        \"D-PSY\", \"NR-PEER\", \"PEER-SOC\", 'PEER-PSY', \"ALC-PEER\", \"D-PEER\", \"PEER\", \"NR\", \"SOC\", \"PSY\", \"ALC\", \"D\"\n",
    "    ]\n",
    "\n",
    "    factor_order.reverse()\n",
    "\n",
    "    df_filtered = df[df[\"Factor_Label\"].isin(factor_order)].copy()\n",
    "    df_filtered[\"Factor_Label\"] = pd.Categorical(df_filtered[\"Factor_Label\"], categories=factor_order, ordered=True)\n",
    "    df_filtered = df_filtered.sort_values(by=\"Factor_Label\", ascending=False)\n",
    "\n",
    "    metrics = [\"auc\", \"f1\", \"balancedAcc\"]\n",
    "    thresholds = {\"auc\": 0.7, \"f1\": 0.31, \"balancedAcc\": 0.5}\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        factor_tuple = tuple(sorted(filter(pd.notna, [row[\"Factor_1\"], row[\"Factor_2\"]])))\n",
    "        perm_values = perm_df[perm_df[\"group\"].apply(lambda x: tuple(sorted(x))) == factor_tuple]\n",
    "        all_test_values = all_test_df[all_test_df[\"group\"].apply(lambda x: tuple(sorted(x))) == factor_tuple]\n",
    "\n",
    "        for metric in metrics:\n",
    "            perm_metric_values = perm_values[metric]\n",
    "            all_metric_values = all_test_values[metric]\n",
    "\n",
    "            perm_p_value = (perm_metric_values >= row[f\"{metric}_mean\"]).sum() / len(perm_metric_values)\n",
    "            t_stat, ttest_p_value = stats.ttest_ind(all_metric_values, perm_metric_values, equal_var=False)\n",
    "\n",
    "            results_list.append({\n",
    "                \"Factor\": row[\"Factor_Label\"],\n",
    "                \"Metric\": metric,\n",
    "                \"Permutation_p\": perm_p_value,\n",
    "                \"TTest_p\": ttest_p_value\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # === Within-domain multiple testing correction ===\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize columns\n",
    "    results_df[\"Permutation_p_FDR_within\"] = np.nan\n",
    "    results_df[\"Permutation_sig_FDR_within\"] = False\n",
    "    results_df[\"TTest_p_FDR_within\"] = np.nan\n",
    "    results_df[\"TTest_sig_FDR_within\"] = False\n",
    "\n",
    "    for fac in results_df[\"Factor\"].unique():\n",
    "        mask = results_df[\"Factor\"] == fac\n",
    "\n",
    "        # Permutation p-values within this domain (across metrics)\n",
    "        p_perm = results_df.loc[mask, \"Permutation_p\"].values\n",
    "        if len(p_perm) > 1:  # apply only if there are multiple tests (e.g., 3 metrics)\n",
    "            rej_perm, p_perm_fdr, _, _ = multipletests(p_perm, alpha=0.05, method=\"fdr_bh\")\n",
    "        else:\n",
    "            rej_perm, p_perm_fdr = np.array([p_perm[0] < 0.05]), p_perm  # trivial case\n",
    "\n",
    "        results_df.loc[mask, \"Permutation_p_FDR_within\"] = p_perm_fdr\n",
    "        results_df.loc[mask, \"Permutation_sig_FDR_within\"] = rej_perm\n",
    "\n",
    "        # T-test p-values within this domain (across metrics)\n",
    "        p_t = results_df.loc[mask, \"TTest_p\"].values\n",
    "        if len(p_t) > 1:\n",
    "            rej_t, p_t_fdr, _, _ = multipletests(p_t, alpha=0.05, method=\"fdr_bh\")\n",
    "        else:\n",
    "            rej_t, p_t_fdr = np.array([p_t[0] < 0.05]), p_t\n",
    "\n",
    "        results_df.loc[mask, \"TTest_p_FDR_within\"] = p_t_fdr\n",
    "        results_df.loc[mask, \"TTest_sig_FDR_within\"] = rej_t\n",
    "    # ================================================================================\n",
    "\n",
    "\n",
    "    # print(results_df)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 10), sharey=True)\n",
    "    plot_settings = [(\"AUC (95% CI)\", \"auc\"), (\"F1 Score (95% CI)\", \"f1\"), (\"Balanced Accuracy (95% CI)\", \"balancedAcc\")]\n",
    "    fig.suptitle(\"SVM (linear) evaluation metrics for nested CV (k=3, j=5, i=100)\", fontsize=20)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 1])\n",
    "    # fig.supxlabel(\"Features Categories\", x=0.01, y=0.5, fontsize=16, rotation=90)\n",
    "\n",
    "    for ax, (title, metric) in zip(axes, plot_settings):\n",
    "        ax.axvline(thresholds[metric], color=\"#4D194D\", linestyle=\"--\")\n",
    "\n",
    "        for i, factor in enumerate(df_filtered[\"Factor_Label\"]):\n",
    "            mean = df_filtered[f\"{metric}_mean\"].iloc[i]\n",
    "            lower = df_filtered[f\"{metric}_CI_lower\"].iloc[i]\n",
    "            upper = df_filtered[f\"{metric}_CI_upper\"].iloc[i]\n",
    "            transparency = 0.5 if lower < thresholds[metric] else 1.0\n",
    "\n",
    "            # ax.scatter(mean, factor, color=\"#065A60\", alpha=transparency, s=50)\n",
    "            # ax.plot([lower, upper], [factor, factor], color=\"#065A60\", alpha=transparency, linewidth=2.5)\n",
    "            is_peer = \"PEER\" in factor\n",
    "            color = \"#4D194D\" if is_peer else \"#065A60\"  # purple for PEER, green otherwise\n",
    "\n",
    "            ax.scatter(mean, factor, color=color, alpha=transparency, s=50)\n",
    "            ax.plot([lower, upper], [factor, factor], color=color, alpha=transparency, linewidth=2.5)\n",
    "\n",
    "            perm_p = results_df[(results_df.Factor == factor) & (results_df.Metric == metric)].Permutation_p_FDR_within.values[0]\n",
    "            if perm_p < 0.05:\n",
    "                ax.text(mean + 0.005, factor, \"*\", color=\"red\", fontsize=16, alpha=transparency)\n",
    "            if perm_p < 0.01:\n",
    "                ax.text(mean + 0.005, factor, \"**\", color=\"red\", fontsize=16, alpha=transparency)\n",
    "\n",
    "        ax.set_xlabel(title, fontsize=16)\n",
    "        ax.tick_params(axis='both', labelsize=16)\n",
    "        ax.grid(True)\n",
    "\n",
    "    star1 = Text(0, 0, '*')\n",
    "    star2 = Text(0, 0, '**')\n",
    "\n",
    "    axes[2].legend([star1, star2],\n",
    "                ['Permutation Test p < 0.05', 'Permutation Test p < 0.01'],\n",
    "                handler_map={star1: HandlerText(), star2: HandlerText()},\n",
    "                loc='upper right', fontsize=14)\n",
    "\n",
    "    # Prepare PEER data for swarm+box plot\n",
    "    grp_tuple = (\"group_sub\",)\n",
    "    perm_grp = perm_df[perm_df[\"group\"].apply(lambda x: tuple(sorted(x))) == grp_tuple].copy()\n",
    "    all_grp = all_test_df[all_test_df[\"group\"].apply(lambda x: tuple(sorted(x))) == grp_tuple].copy()\n",
    "\n",
    "    perm_grp[\"type\"] = \"Permuted\"\n",
    "    all_grp[\"type\"] = \"Real\"\n",
    "\n",
    "    combined = pd.concat([perm_grp, all_grp], ignore_index=True)\n",
    "\n",
    "    # Melt dataframe for seaborn\n",
    "    melted = combined.melt(id_vars=[\"type\"], value_vars=metrics, var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=melted, x=\"Metric\", y=\"Score\", hue=\"type\", fliersize=0, width=0.6, palette=[\"#C08497\", \"#88D9E6\"], boxprops=dict(facecolor='none'))\n",
    "    sns.swarmplot(data=melted, x=\"Metric\", y=\"Score\", hue=\"type\", dodge=True, palette=[\"#C08497\", \"#88D9E6\"], alpha=0.7, size=4)\n",
    "\n",
    "    plt.title(\"Real vs. Permuted Test Metric Distributions for PEER\", fontsize=16)\n",
    "    plt.xlabel(\"Metric\", fontsize=14)\n",
    "    plt.ylabel(\"Score\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[:2], labels[:2], title=\"Data Type\", fontsize=12, title_fontsize=13)\n",
    "\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(\n",
    "    res_dir=RES_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save formatted final results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original CSV\n",
    "df = pd.read_csv(os.path.join(RES_DIR, \"test_scores.csv\")) \n",
    "\n",
    "# Calculate PPV + CIs before formatting\n",
    "df[\"PPV_mean\"] = df[\"tp_mean\"] / (df[\"tp_mean\"] + df[\"fp_mean\"])\n",
    "df[\"PPV_CI_lower\"] = df[\"tp_CI_lower\"] / (df[\"tp_CI_lower\"] + df[\"fp_CI_upper\"])\n",
    "df[\"PPV_CI_upper\"] = df[\"tp_CI_upper\"] / (df[\"tp_CI_upper\"] + df[\"fp_CI_lower\"])\n",
    "\n",
    "\n",
    "# Metrics to include and format\n",
    "selected_metrics = [\"auc\", \"f1\", \"balancedAcc\", \"sensitivity\", \"specificity\", \"pr_auc\", \"PPV\", \"NPV\"]\n",
    "\n",
    "# Build formatted output\n",
    "output_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    row_dict = {}\n",
    "\n",
    "    # Combine Factor_1 and Factor_2 into \"Feature Category\"\n",
    "    factor_1 = row['Factor_1']\n",
    "    factor_2 = row['Factor_2']\n",
    "    if pd.isna(factor_2) or str(factor_2).strip() == \"\":\n",
    "        row_dict[\"Feature Category\"] = str(factor_1)\n",
    "    else:\n",
    "        row_dict[\"Feature Category\"] = f\"{factor_1}-{factor_2}\"\n",
    "\n",
    "    # Format selected metrics with newline and 3 decimal precision\n",
    "    for metric in selected_metrics:\n",
    "        mean = f\"{row[f'{metric}_mean']:.3f}\"\n",
    "        ci_low = f\"{row[f'{metric}_CI_lower']:.3f}\"\n",
    "        ci_up = f\"{row[f'{metric}_CI_upper']:.3f}\"\n",
    "        row_dict[metric.upper()] = f\"{mean}\\n({ci_low}â€“{ci_up})\"\n",
    "\n",
    "    output_rows.append(row_dict)\n",
    "\n",
    "# Create and save the final DataFrame\n",
    "df_out = pd.DataFrame(output_rows)\n",
    "\n",
    "# Save to new CSV\n",
    "df_out.to_csv(os.path.join(RES_DIR, \"formatted_metrics_table.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual vs Perceived Peer Drinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vs_perceived = pd.read_csv('../data/added_analysis/peer_perceptions_vs_peer_selfreports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = true_vs_perceived\n",
    "cols = [\"avg_amount_friends_true\", \"avg_alcmost_amount\",\n",
    "        \"avg_freq_friends_true\", \"avg_alcmost_freq\"]\n",
    "for c in cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=cols)\n",
    "\n",
    "# Convert yearly frequency to weekly on the copy only\n",
    "df[\"avg_freq_true_week\"] = df[\"avg_freq_friends_true\"] / 52.0\n",
    "df[\"avg_freq_perc_week\"] = df[\"avg_alcmost_freq\"] / 52.0\n",
    "\n",
    "# Figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "label_fs, title_fs, tick_fs = 14, 16, 12\n",
    "\n",
    "# Panel A: Amount per occasion\n",
    "sns.scatterplot(\n",
    "    data=df, x=\"avg_amount_friends_true\", y=\"avg_alcmost_amount\",\n",
    "    ax=axes[0], s=40, alpha=0.6, edgecolor=None\n",
    ")\n",
    "# Set symmetric limits so 1:1 line is meaningful\n",
    "max_a = np.nanmax(df[[\"avg_amount_friends_true\",\"avg_alcmost_amount\"]].values)\n",
    "max_a = max(1.0, min(max_a, 10.0))  # cap at 10 if you want the plot bounded\n",
    "axes[0].plot([0, max_a], [0, max_a], color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0].set_xlim(0, max_a); axes[0].set_ylim(0, max_a)\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "axes[0].set_xlabel(\"Peers' self-reported number of drinks per occasion\", fontsize=label_fs)\n",
    "axes[0].set_ylabel(\"Perceived number of drinks per occasion\", fontsize=label_fs)\n",
    "axes[0].set_title(\"A. Perceived vs Self-Reported Drinking Amount\", fontsize=title_fs)\n",
    "axes[0].grid(True, linestyle=\":\", alpha=0.4)\n",
    "axes[0].tick_params(axis='both', labelsize=tick_fs)\n",
    "\n",
    "# Panel B: Weekly frequency\n",
    "sns.scatterplot(\n",
    "    data=df, x=\"avg_freq_true_week\", y=\"avg_freq_perc_week\",\n",
    "    ax=axes[1], s=40, alpha=0.6, edgecolor=None\n",
    ")\n",
    "max_f = np.nanmax(df[[\"avg_freq_true_week\",\"avg_freq_perc_week\"]].values)\n",
    "# reasonable upper bound for weekly occasions\n",
    "max_f = max(0.5, min(max_f, 7.0))\n",
    "axes[1].plot([0, max_f], [0, max_f], color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[1].set_xlim(0, max_f); axes[1].set_ylim(0, max_f)\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "axes[1].set_xlabel(\"Peers' self-reported drinking occasions per week\", fontsize=label_fs)\n",
    "axes[1].set_ylabel(\"Perceived drinking occasions per week\", fontsize=label_fs)\n",
    "axes[1].set_title(\"B. Perceived vs Self-Reported Weekly Drinking Frequency\", fontsize=title_fs)\n",
    "axes[1].grid(True, linestyle=\":\", alpha=0.4)\n",
    "axes[1].tick_params(axis='both', labelsize=tick_fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance_comparison(res_dir_rf, res_dir_lr):\n",
    "    \"\"\"\n",
    "    Plots model performance comparing Random Forest vs Logistic Regression\n",
    "    across AUC, F1, and Balanced Accuracy, including permutation significance markers,\n",
    "    clean CI lines (no caps), shaded PEER-related categories, and legend with red asterisks.\n",
    "    \"\"\"\n",
    "    import os, ast\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "    from scipy import stats\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "    def load_data(res_dir):\n",
    "        perm_folder = os.path.join(res_dir, 'permutation_test')\n",
    "        subfolders = [f for f in os.listdir(perm_folder) if os.path.isdir(os.path.join(perm_folder, f))]\n",
    "        assert len(subfolders) == 1, f\"Expected exactly one subfolder in {perm_folder}\"\n",
    "        perm_folder = os.path.join(perm_folder, subfolders[0])\n",
    "\n",
    "        df = pd.read_csv(os.path.join(res_dir, \"test_scores.csv\"))\n",
    "        perm_df = pd.read_csv(os.path.join(perm_folder, \"all_test_scores.csv\"))\n",
    "        all_test_df = pd.read_csv(os.path.join(res_dir, \"all_test_scores.csv\"))\n",
    "\n",
    "        perm_df[\"group\"] = perm_df[\"group\"].apply(ast.literal_eval)\n",
    "        all_test_df[\"group\"] = all_test_df[\"group\"].apply(ast.literal_eval)\n",
    "\n",
    "        factor_map = {\n",
    "            \"group_sub\": \"PEER\", \"alc_self\": \"ALC\", \"psych\": \"PSY\",\n",
    "            \"group_socio\": \"SOC\", \"brain\": \"NR\", \"demo\": \"D\"\n",
    "        }\n",
    "        df[\"Factor_Label\"] = df.apply(\n",
    "            lambda row: \"-\".join(sorted([factor_map.get(f, f) for f in [row[\"Factor_1\"], row[\"Factor_2\"]] if pd.notna(f)])),\n",
    "            axis=1\n",
    "        )\n",
    "        return df, perm_df, all_test_df\n",
    "\n",
    "    # === Load both models ===\n",
    "    df_rf, perm_df_rf, all_test_df_rf = load_data(res_dir_rf)\n",
    "    df_lr, perm_df_lr, all_test_df_lr = load_data(res_dir_lr)\n",
    "\n",
    "    factor_order = [\n",
    "        \"PSY-SOC\", \"NR-SOC\", \"NR-PSY\", \"ALC-NR\", \"ALC-SOC\", \"ALC-PSY\",\n",
    "        \"ALC-D\", \"D-NR\", \"D-SOC\", \"D-PSY\", \"NR-PEER\", \"PEER-SOC\", \"PEER-PSY\",\n",
    "        \"ALC-PEER\", \"D-PEER\", \"PEER\", \"NR\", \"SOC\", \"PSY\", \"ALC\", \"D\"\n",
    "    ][::-1]\n",
    "\n",
    "    def prepare_df(df):\n",
    "        df = df[df[\"Factor_Label\"].isin(factor_order)].copy()\n",
    "        df[\"Factor_Label\"] = pd.Categorical(df[\"Factor_Label\"], categories=factor_order, ordered=True)\n",
    "        df = df.sort_values(by=\"Factor_Label\", ascending=False)\n",
    "        return df\n",
    "\n",
    "    df_rf = prepare_df(df_rf)\n",
    "    df_lr = prepare_df(df_lr)\n",
    "\n",
    "    metrics = [\"auc\", \"f1\", \"balancedAcc\"]\n",
    "    thresholds = {\"auc\": 0.7, \"f1\": 0.31, \"balancedAcc\": 0.5}\n",
    "\n",
    "    def compute_significance(df_main, perm_df, all_test_df):\n",
    "        results_list = []\n",
    "        for _, row in df_main.iterrows():\n",
    "            factor_tuple = tuple(sorted(filter(pd.notna, [row[\"Factor_1\"], row[\"Factor_2\"]])))\n",
    "            perm_values = perm_df[perm_df[\"group\"].apply(lambda x: tuple(sorted(x))) == factor_tuple]\n",
    "            for metric in metrics:\n",
    "                perm_metric_values = perm_values[metric]\n",
    "                perm_p_value = (perm_metric_values >= row[f\"{metric}_mean\"]).sum() / len(perm_metric_values)\n",
    "                results_list.append({\n",
    "                    \"Factor\": row[\"Factor_Label\"],\n",
    "                    \"Metric\": metric,\n",
    "                    \"Permutation_p\": perm_p_value\n",
    "                })\n",
    "        results_df = pd.DataFrame(results_list)\n",
    "        results_df[\"Permutation_p_FDR_within\"] = np.nan\n",
    "        results_df[\"Permutation_sig_FDR_within\"] = False\n",
    "        for fac in results_df[\"Factor\"].unique():\n",
    "            mask = results_df[\"Factor\"] == fac\n",
    "            p_vals = results_df.loc[mask, \"Permutation_p\"].values\n",
    "            if len(p_vals) > 1:\n",
    "                rej, p_fdr, _, _ = multipletests(p_vals, alpha=0.05, method=\"fdr_bh\")\n",
    "            else:\n",
    "                rej, p_fdr = np.array([p_vals[0] < 0.05]), p_vals\n",
    "            results_df.loc[mask, \"Permutation_p_FDR_within\"] = p_fdr\n",
    "            results_df.loc[mask, \"Permutation_sig_FDR_within\"] = rej\n",
    "        return results_df\n",
    "\n",
    "    sig_rf = compute_significance(df_rf, perm_df_rf, all_test_df_rf)\n",
    "    sig_lr = compute_significance(df_lr, perm_df_lr, all_test_df_lr)\n",
    "\n",
    "    # === Plotting ===\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 10), sharey=True)\n",
    "    plot_settings = [(\"AUC (95% CI)\", \"auc\"), (\"F1 Score (95% CI)\", \"f1\"), (\"Balanced Accuracy (95% CI)\", \"balancedAcc\")]\n",
    "    fig.suptitle(\"Random forest and logistic regression evaluation metrics for nested CV (k=3, j=5, i=100)\", fontsize=20)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    colors = {\"RF\": \"#065A60\", \"LR\": \"#C08497\"}\n",
    "\n",
    "    # Identify indices for PEER-related rows\n",
    "    peer_indices = [i for i, f in enumerate(df_rf[\"Factor_Label\"]) if \"PEER\" in f]\n",
    "\n",
    "    for ax, (title, metric) in zip(axes, plot_settings):\n",
    "        ax.axvline(thresholds[metric], color=\"gray\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Shade PEER-related rows\n",
    "        for idx in peer_indices:\n",
    "            ax.axhspan(idx - 0.5, idx + 0.5, color=\"#BFD9DA\", alpha=0.4, zorder=0)\n",
    "\n",
    "        for model_label, df_model, sig_df, color in [\n",
    "            (\"RF\", df_rf, sig_rf, colors[\"RF\"]),\n",
    "            (\"LR\", df_lr, sig_lr, colors[\"LR\"])\n",
    "        ]:\n",
    "            offset = -0.2 if model_label == \"RF\" else 0.2\n",
    "            for i, factor in enumerate(df_model[\"Factor_Label\"]):\n",
    "                mean = df_model[f\"{metric}_mean\"].iloc[i]\n",
    "                lower = df_model[f\"{metric}_CI_lower\"].iloc[i]\n",
    "                upper = df_model[f\"{metric}_CI_upper\"].iloc[i]\n",
    "                transparency = 0.5 if lower < thresholds[metric] else 1.0\n",
    "\n",
    "                # Clean CI lines (no caps)\n",
    "                ax.errorbar(mean, i + offset, xerr=[[mean - lower], [upper - mean]], fmt='o',\n",
    "                            color=color, alpha=transparency, label=model_label if i == 0 else \"\",\n",
    "                            capsize=0, elinewidth=2.5, solid_capstyle='butt', markersize=6)\n",
    "\n",
    "                # Significance markers\n",
    "                perm_p = sig_df[(sig_df.Factor == factor) & (sig_df.Metric == metric)].Permutation_p_FDR_within.values[0]\n",
    "                if perm_p < 0.01:\n",
    "                    ax.text(mean + 0.005, i + offset - 0.1, \"**\", color=\"red\", fontsize=14, alpha=transparency)\n",
    "                elif perm_p < 0.05:\n",
    "                    ax.text(mean + 0.005, i + offset - 0.1, \"*\", color=\"red\", fontsize=14, alpha=transparency)\n",
    "\n",
    "        ax.set_xlabel(title, fontsize=16)\n",
    "        ax.set_yticks(range(len(df_rf[\"Factor_Label\"])))\n",
    "        ax.set_yticklabels(df_rf[\"Factor_Label\"], fontsize=14)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.5, zorder=1)\n",
    "\n",
    "    # === Legend ===\n",
    "    handles_model = [\n",
    "        Line2D([0], [0], color=colors[\"RF\"], marker='o', linestyle='', label='Random Forest'),\n",
    "        Line2D([0], [0], color=colors[\"LR\"], marker='o', linestyle='', label='Logistic Regression')\n",
    "    ]\n",
    "    handles_sig = [\n",
    "        Line2D([0], [0], color='red', marker='', linestyle='', label='*  p < 0.05'),\n",
    "        Line2D([0], [0], color='red', marker='', linestyle='', label='** p < 0.01')\n",
    "    ]\n",
    "\n",
    "    axes[2].legend(handles=handles_model + handles_sig, loc='upper right', fontsize=14, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance_comparison(\"../results/1750853719_321_rf_outer100_cvrep5_k3_auc_testsize0.3_permFalse_og\", \"../results_logreg/1760898281_321_lr_elasticnet_outer100_cvrep5_k3_auc_testsize0.3_permFalse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
